import numpy as np
import cv2
import tensorrt as trt
import pycuda.driver as cuda
import pycuda.autoinit
import time
import json

import os
import json
import cv2
import numpy as np

from pathlib import Path
from dotenv import load_dotenv
from kafka_util import consumers, producers
from deep_sort_realtime.deepsort_tracker import DeepSort

class Detector_and_tracker(self):
    def __init__(self, class_names_path=None, conf_threshold=0.25, iou_threshold=0.45, cameraID=None):
        """
        TensorRT YOLOv11 ì—”ì§„ ì´ˆê¸°í™”
        
        Args:
            class_names_path: í´ë˜ìŠ¤ ì´ë¦„ íŒŒì¼ ê²½ë¡œ (ì„ íƒì‚¬í•­)
            conf_threshold: ì‹ ë¢°ë„ ì„ê³„ê°’
            iou_threshold: IoU ì„ê³„ê°’
        """
        # initialize kafka module
        self.frame_consumer = consumers.FrameConsumer()
        self.result_producer = producers.DetectedResultProducer()   
        self.track_result_producer = producers.TrackResultProducer()

        # initialize local id assignment
        self.local_id_set = set()

        # load engine
        self.engine_path = os.getenv('ENGINE_PATH')
        self.conf_threshold = conf_threshold
        self.iou_threshold = iou_threshold
        
        # í´ë˜ìŠ¤ ì´ë¦„ ë¡œë“œ
        self.class_names = self._load_class_names(class_names_path)
        self.colors = np.random.uniform(0, 255, size=(len(self.class_names), 3))
        
        # TensorRT ì—”ì§„ ë¡œë“œ
        self.logger = trt.Logger(trt.Logger.WARNING)
        self.engine = self._load_engine()
        self.context = self.engine.create_execution_context()
        
        # ì…ì¶œë ¥ ë°”ì¸ë”© ì„¤ì •
        self.inputs, self.outputs, self.bindings, self.stream = self._allocate_buffers()
        
        # initialize tracker 
        self.tracker = DeepSort(max_age=5)

        # ëª¨ë¸ ì •ë³´ ì¶œë ¥
        self._print_engine_info()
    
    def _load_class_names(self, class_names_path):
        """í´ë˜ìŠ¤ ì´ë¦„ ë¡œë“œ"""
        if class_names_path and Path(class_names_path).exists():
            try:
                with open(class_names_path, 'r', encoding='utf-8') as f:
                    if class_names_path.endswith('.json'):
                        data = json.load(f)
                        return data.get('names', list(data.values()))
                    else:
                        return [line.strip() for line in f.readlines() if line.strip()]
            except Exception as e:
                print(f"âš ï¸  í´ë˜ìŠ¤ íŒŒì¼ ë¡œë“œ ì‹¤íŒ¨: {e}")
        
        # COCO 80ê°œ í´ë˜ìŠ¤ (YOLOv11 ê¸°ë³¸)
        return [
            'person', 'bicycle', 'car', 'motorcycle', 'airplane', 'bus', 'train', 'truck', 'boat',
            'traffic light', 'fire hydrant', 'stop sign', 'parking meter', 'bench', 'bird', 'cat',
            'dog', 'horse', 'sheep', 'cow', 'elephant', 'bear', 'zebra', 'giraffe', 'backpack',
            'umbrella', 'handbag', 'tie', 'suitcase', 'frisbee', 'skis', 'snowboard', 'sports ball',
            'kite', 'baseball bat', 'baseball glove', 'skateboard', 'surfboard', 'tennis racket',
            'bottle', 'wine glass', 'cup', 'fork', 'knife', 'spoon', 'bowl', 'banana', 'apple',
            'sandwich', 'orange', 'broccoli', 'carrot', 'hot dog', 'pizza', 'donut', 'cake',
            'chair', 'couch', 'potted plant', 'bed', 'dining table', 'toilet', 'tv', 'laptop',
            'mouse', 'remote', 'keyboard', 'cell phone', 'microwave', 'oven', 'toaster', 'sink',
            'refrigerator', 'book', 'clock', 'vase', 'scissors', 'teddy bear', 'hair drier',
            'toothbrush'
        ]
    
    def _load_engine(self):
        """TensorRT ì—”ì§„ ë¡œë“œ"""
        try:
            with open(self.engine_path, 'rb') as f, trt.Runtime(self.logger) as runtime:
                engine = runtime.deserialize_cuda_engine(f.read())
            print(f"âœ… TensorRT ì—”ì§„ ë¡œë“œ ì™„ë£Œ: {self.engine_path}")
            return engine
        except Exception as e:
            print(f"âŒ ì—”ì§„ ë¡œë“œ ì‹¤íŒ¨: {e}")
            raise
    
    def _allocate_buffers(self):
        """GPU ë©”ëª¨ë¦¬ í• ë‹¹"""
        inputs = []
        outputs = []
        bindings = []
        stream = cuda.Stream()
        
        # TensorRT ë²„ì „ì— ë”°ë¥¸ ì²˜ë¦¬
        if hasattr(self.engine, 'num_io_tensors'):
            # TensorRT 8.5+ ìƒˆë¡œìš´ API
            for i in range(self.engine.num_io_tensors):
                tensor_name = self.engine.get_tensor_name(i)
                dtype = trt.nptype(self.engine.get_tensor_dtype(tensor_name))
                shape = self.context.get_tensor_shape(tensor_name)
                size = trt.volume(shape)
                
                # GPU ë©”ëª¨ë¦¬ í• ë‹¹
                host_mem = cuda.pagelocked_empty(size, dtype)
                device_mem = cuda.mem_alloc(host_mem.nbytes)
                bindings.append(int(device_mem))
                
                if self.engine.get_tensor_mode(tensor_name) == trt.TensorIOMode.INPUT:
                    self.input_shape = shape
                    self.input_name = tensor_name
                    inputs.append({'host': host_mem, 'device': device_mem, 'name': tensor_name})
                else:
                    self.output_name = tensor_name
                    outputs.append({'host': host_mem, 'device': device_mem, 'shape': shape, 'name': tensor_name})
        else:
            # TensorRT 7.x/8.x ì´ì „ API
            for binding in self.engine:
                dtype = trt.nptype(self.engine.get_binding_dtype(binding))
                shape = self.context.get_binding_shape(binding)
                size = trt.volume(shape)
                
                # GPU ë©”ëª¨ë¦¬ í• ë‹¹
                host_mem = cuda.pagelocked_empty(size, dtype)
                device_mem = cuda.mem_alloc(host_mem.nbytes)
                bindings.append(int(device_mem))
                
                if self.engine.binding_is_input(binding):
                    self.input_shape = shape
                    self.input_name = binding
                    inputs.append({'host': host_mem, 'device': device_mem, 'name': binding})
                else:
                    self.output_name = binding
                    outputs.append({'host': host_mem, 'device': device_mem, 'shape': shape, 'name': binding})
        
        return inputs, outputs, bindings, stream
    
    def _print_engine_info(self):
        """ì—”ì§„ ì •ë³´ ì¶œë ¥"""
        print(f"ğŸ”§ TensorRT ì—”ì§„ ì •ë³´:")
        print(f"   - TensorRT ë²„ì „: {trt.__version__}")
        print(f"   - ì…ë ¥ í¬ê¸°: {self.input_shape}")
        print(f"   - í´ë˜ìŠ¤ ìˆ˜: {len(self.class_names)}")
        print(f"   - ì‹ ë¢°ë„ ì„ê³„ê°’: {self.conf_threshold}")
        print(f"   - IoU ì„ê³„ê°’: {self.iou_threshold}")
        
        # ì‚¬ìš© ê°€ëŠ¥í•œ ë©”ì„œë“œ í™•ì¸
        methods = []
        if hasattr(self.context, 'execute_async_v3'):
            methods.append("execute_async_v3")
        if hasattr(self.context, 'execute_async_v2'):
            methods.append("execute_async_v2")
        if hasattr(self.context, 'execute_async'):
            methods.append("execute_async")
        if hasattr(self.context, 'execute_v2'):
            methods.append("execute_v2")
        if hasattr(self.context, 'execute'):
            methods.append("execute")
        
        print(f"   - ì‚¬ìš© ê°€ëŠ¥í•œ ì‹¤í–‰ ë©”ì„œë“œ: {', '.join(methods)}")
    
    def preprocess(self, image):
        """ì´ë¯¸ì§€ ì „ì²˜ë¦¬"""
        # YOLOv11 ì…ë ¥ í¬ê¸° (ë³´í†µ 640x640)
        input_h, input_w = self.input_shape[2], self.input_shape[3]
        
        # ì´ë¯¸ì§€ ë¦¬ì‚¬ì´ì¦ˆ (ë¹„ìœ¨ ìœ ì§€)
        img_h, img_w = image.shape[:2]
        scale = min(input_w / img_w, input_h / img_h)
        new_w, new_h = int(img_w * scale), int(img_h * scale)
        
        # ë¦¬ì‚¬ì´ì¦ˆ ë° íŒ¨ë”©
        # ì…ì¶œë ¥ í¬ê¸°ë¥¼ ë§ì¶”ë©´ preprocessingì´ í•„ìš”í•œê°€?
        resized = cv2.resize(image, (new_w, new_h))
        padded = np.full((input_h, input_w, 3), 114, dtype=np.uint8)
        
        # ì¤‘ì•™ ë°°ì¹˜
        pad_x = (input_w - new_w) // 2
        pad_y = (input_h - new_h) // 2
        padded[pad_y:pad_y + new_h, pad_x:pad_x + new_w] = resized
        
        # ì •ê·œí™” ë° ì°¨ì› ë³€ê²½ (HWC -> CHW)
        input_tensor = padded.astype(np.float32) / 255.0
        input_tensor = np.transpose(input_tensor, (2, 0, 1))
        input_tensor = np.expand_dims(input_tensor, axis=0)
        
        return input_tensor, scale, pad_x, pad_y
    
    def postprocess(self, outputs, scale, pad_x, pad_y, debug=False):
        """í›„ì²˜ë¦¬ - NMS ì ìš©í•˜ì—¬ ìµœì¢… ê²€ì¶œ ê²°ê³¼ ìƒì„±"""
        if debug:
            print(f"ğŸ” í›„ì²˜ë¦¬ ë””ë²„ê·¸:")
            print(f"   - ì¶œë ¥ ê°œìˆ˜: {len(outputs)}")
            for i, output in enumerate(outputs):
                print(f"   - ì¶œë ¥ {i} í˜•íƒœ: {output.shape}")
        
        # YOLOv11 ì¶œë ¥ í˜•ì‹ í™•ì¸ ë° ì²˜ë¦¬
        output = outputs[0]
        
        # YOLOv11ì€ ë³´í†µ (1, 84, 8400) í˜•ì‹ìœ¼ë¡œ ì¶œë ¥ë¨
        if len(output.shape) == 3:
            output = output[0]  # ë°°ì¹˜ ì°¨ì› ì œê±° -> (84, 8400)
            if output.shape[0] < output.shape[1]:  # (84, 8400) -> (8400, 84)ë¡œ ì „ì¹˜
                output = output.T
        
        if debug:
            print(f"   - ì²˜ë¦¬ëœ ì¶œë ¥ í˜•íƒœ: {output.shape}")
            print(f"   - ì‹ ë¢°ë„ ì„ê³„ê°’: {self.conf_threshold}")
        
        boxes = []
        scores = []
        class_ids = []
        
        # YOLOv11 ì¶œë ¥ íŒŒì‹±
        for i, detection in enumerate(output):
            # YOLOv11 í˜•ì‹: [x_center, y_center, width, height, class0_conf, class1_conf, ...]
            x_center, y_center, width, height = detection[:4]
            class_confs = detection[4:]
            
            # ìµœëŒ€ í´ë˜ìŠ¤ ì‹ ë¢°ë„ ì°¾ê¸°
            max_conf = np.max(class_confs)
            class_id = np.argmax(class_confs)
            
            if debug and i < 5:  # ì²˜ìŒ 5ê°œë§Œ ë””ë²„ê·¸ ì¶œë ¥
                print(f"   - ê²€ì¶œ {i}: conf={max_conf:.3f}, class={class_id}, pos=({x_center:.1f},{y_center:.1f})")
            
            if max_conf > self.conf_threshold:
                # ì¢Œí‘œ ë³€í™˜ (ëª¨ë¸ ì…ë ¥ í¬ê¸° ê¸°ì¤€ -> ì›ë³¸ ì´ë¯¸ì§€ ê¸°ì¤€)
                input_h, input_w = self.input_shape[2], self.input_shape[3]
                
                # ì •ê·œí™”ëœ ì¢Œí‘œë¥¼ í”½ì…€ ì¢Œí‘œë¡œ ë³€í™˜
                x_center = x_center
                y_center = y_center
                width = width
                height = height
                
                # íŒ¨ë”© ë³´ì • ë° ìŠ¤ì¼€ì¼ë§
                x_center = (x_center - pad_x) / scale
                y_center = (y_center - pad_y) / scale
                width = width / scale
                height = height / scale
                
                # ì¤‘ì‹¬ì ì„ ì¢Œìƒë‹¨ ì¢Œí‘œë¡œ ë³€í™˜
                x1 = x_center - width / 2
                y1 = y_center - height / 2
                
                boxes.append([x1, y1, width, height])
                scores.append(float(max_conf))
                class_ids.append(int(class_id))
        
        if debug:
            print(f"   - ì„ê³„ê°’ í†µê³¼ ê°ì²´: {len(boxes)}ê°œ")
        
        if len(boxes) == 0:
            return [], [], []
        
        # NMS ì ìš©
        boxes = np.array(boxes)
        scores = np.array(scores)
        class_ids = np.array(class_ids)
        
        indices = cv2.dnn.NMSBoxes(boxes.tolist(), scores.tolist(), 
                                  self.conf_threshold, self.iou_threshold)
        
        if len(indices) > 0:
            if isinstance(indices, tuple):
                indices = indices[0] if len(indices) > 0 else []
            if len(indices) > 0:
                indices = indices.flatten() if hasattr(indices, 'flatten') else indices
                
                if debug:
                    print(f"   - NMS í›„ ìµœì¢… ê°ì²´: {len(indices)}ê°œ")
                
                return boxes[indices], scores[indices], class_ids[indices]
        
        if debug:
            print(f"   - NMS í›„ ìµœì¢… ê°ì²´: 0ê°œ")
        
        return [], [], []
    
    def infer(self, image, debug=False):
        """ì¶”ë¡  ì‹¤í–‰"""
        # ì „ì²˜ë¦¬
        start_time = time.time()
        input_tensor, scale, pad_x, pad_y = self.preprocess(image)
        preprocess_time = time.time() - start_time
        
        if debug:
            print(f"ğŸ” ì¶”ë¡  ë””ë²„ê·¸:")
            print(f"   - ì…ë ¥ ì´ë¯¸ì§€ í¬ê¸°: {image.shape}")
            print(f"   - ì „ì²˜ë¦¬ëœ í…ì„œ í¬ê¸°: {input_tensor.shape}")
            print(f"   - ìŠ¤ì¼€ì¼: {scale:.3f}, íŒ¨ë”©: ({pad_x}, {pad_y})")
        
        # GPUë¡œ ë°ì´í„° ë³µì‚¬
        start_time = time.time()
        np.copyto(self.inputs[0]['host'], input_tensor.ravel())
        cuda.memcpy_htod_async(self.inputs[0]['device'], self.inputs[0]['host'], self.stream)
        
        # TensorRT ë²„ì „ì— ë”°ë¥¸ ì¶”ë¡  ì‹¤í–‰ (ìµœì í™”ëœ ë²„ì „)
        try:
            # execute_async_v3 ìš°ì„  ì‚¬ìš© (ê°€ì¥ ë¹ ë¦„)
            if hasattr(self.context, 'execute_async_v3'):
                # í…ì„œ ì£¼ì†Œ ì„¤ì •
                self.context.set_tensor_address(self.input_name, self.inputs[0]['device'])
                for output in self.outputs:
                    self.context.set_tensor_address(output['name'], output['device'])
                # ë¹„ë™ê¸° ì‹¤í–‰
                self.context.execute_async_v3(stream_handle=self.stream.handle)
            else:
                # í´ë°±: execute_v2 ì‚¬ìš© (ë™ê¸°)
                self.context.execute_v2(bindings=self.bindings)
        except Exception as e:
            print(f"ì¶”ë¡  ì‹¤í–‰ ì˜¤ë¥˜: {e}")
            # ìµœì¢… í´ë°±
            self.context.execute_v2(bindings=self.bindings)
        
        # ê²°ê³¼ë¥¼ CPUë¡œ ë³µì‚¬
        for output in self.outputs:
            cuda.memcpy_dtoh_async(output['host'], output['device'], self.stream)
        
        self.stream.synchronize()
        inference_time = time.time() - start_time
        
        # í›„ì²˜ë¦¬
        start_time = time.time()
        output_data = [output['host'].reshape(output['shape']) for output in self.outputs]
        
        if debug:
            print(f"   - ì¶”ë¡  ì‹œê°„: {inference_time*1000:.1f}ms")
            for i, data in enumerate(output_data):
                print(f"   - ì¶œë ¥ {i} í†µê³„: min={np.min(data):.3f}, max={np.max(data):.3f}, mean={np.mean(data):.3f}")
        
        boxes, scores, class_ids = self.postprocess(output_data, scale, pad_x, pad_y, debug)
        postprocess_time = time.time() - start_time
        
        return boxes, scores, class_ids, {
            'preprocess': preprocess_time,
            'inference': inference_time,
            'postprocess': postprocess_time,
            'total': preprocess_time + inference_time + postprocess_time
        }
    
    def extract_crop_from_frame(self, frame: np.ndarray, bbox: list) -> str:
        """Extract person crop from frame and encode to base64"""
        try:
            x, y, w, h = bbox
            crop = frame[y:y+h, x:x+w]
            
            # Convert to PIL Image
            crop_pil = Image.fromarray(cv2.cvtColor(crop, cv2.COLOR_BGR2RGB))
            
            # Encode to base64
            buffer = io.BytesIO()
            crop_pil.save(buffer, format='JPEG')
            crop_b64 = base64.b64encode(buffer.getvalue()).decode('utf-8')
            
            return crop_b64
        except Exception as e:
            self.logger.error(f"Failed to extract crop: {e}")
            return ""
        
    
    def detect_and_track(self, frame, debug=False):
        # 0) Inference
        boxes, scores, class_ids, timing_info = self.infer(frame, debug)
        # timing_info í™œìš© ì˜ˆì‹œ
        if debug:
            print(f"Inference timing: {timing_info}")

        # 1) DeepSort í˜•ì‹ìœ¼ë¡œ ë³€í™˜
        detections = []
        for box, score, class_id in zip(boxes, scores, class_ids):
            x1, y1, w, h = box.astype(int)
            detections.append(
                ([x1, y1, w, h], float(score), int(class_id))
            )

        # 2) Crop & Embedding (í•œ ë²ˆë§Œ ìˆ˜í–‰)
        object_chips = [
            frame[y:y+h, x:x+w]
            for (x, y, w, h), _, _ in detections
        ]
        embeds = self.embedder(object_chips)

        # 3) Track ì—…ë°ì´íŠ¸
        tracks = self.tracker.update_tracks(
            detections,
            embeds=embeds,
            frame=frame   # ì‹œê°í™” ìš©ë„
        )

        # 4) Confirmedëœ ì‹ ê·œ trackë§Œ ReID ìš”ì²­
        for track in tracks:
            if not track.is_confirmed():
                continue

            track_id = track.track_id
            if track_id in self.local_id_set:
                continue

            # bbox ì¢Œí‘œ (Left, Top, Right, Bottom)
            l, t, r, b = track.to_ltrb()
            crop = frame[t:b, l:r]

            # ReID ìš”ì²­ (track_idì™€ crop ì´ë¯¸ì§€ ì „ë‹¬)
            self.track_result_producer.send_message(crop)

            # ì²˜ë¦¬ëœ ID ê¸°ë¡
            self.local_id_set.add(track_id)

        
    
    # TODO: ì¼ì • ì£¼ê¸°ë¡œ draw í•˜ê³  í”„ë ˆì„ ìì²´ë¥¼ ì „ì†¡í•˜ë„ë¡  ìˆ˜ì •í•´ì•¼í•¨.
    def draw_detections(self, image, boxes, scores, class_ids, debug=False):
        """ê²€ì¶œ ê²°ê³¼ë¥¼ ì´ë¯¸ì§€ì— ê·¸ë¦¬ê¸°"""
        if debug:
            print(f"ğŸ¨ ê·¸ë¦¬ê¸° ë””ë²„ê·¸:")
            print(f"   - ì´ë¯¸ì§€ í¬ê¸°: {image.shape}")
            print(f"   - ë°•ìŠ¤ ê°œìˆ˜: {len(boxes)}")
        
        if len(boxes) == 0:
            if debug:
                print("   - ê·¸ë¦´ ë°•ìŠ¤ê°€ ì—†ìŒ")
            return image
        
        result_image = image.copy()
        
        for i, (box, score, class_id) in enumerate(zip(boxes, scores, class_ids)):
            try:
                x1, y1, w, h = box.astype(int)
                x2, y2 = x1 + w, y1 + h
                
                # ì´ë¯¸ì§€ ê²½ê³„ ë‚´ë¡œ ì œí•œ
                img_h, img_w = image.shape[:2]
                x1 = max(0, min(x1, img_w - 1))
                y1 = max(0, min(y1, img_h - 1))
                x2 = max(0, min(x2, img_w - 1))
                y2 = max(0, min(y2, img_h - 1))
                
                if debug and i < 3:
                    print(f"   - ë°•ìŠ¤ {i}: ({x1},{y1})-({x2},{y2}), í´ë˜ìŠ¤={class_id}, ì ìˆ˜={score:.3f}")
                
                # ë°•ìŠ¤ê°€ ë„ˆë¬´ ì‘ìœ¼ë©´ ìŠ¤í‚µ
                if x2 - x1 < 5 or y2 - y1 < 5:
                    continue
                
                # í´ë˜ìŠ¤ ì´ë¦„ê³¼ ìƒ‰ìƒ
                class_name = self.class_names[class_id] if class_id < len(self.class_names) else f"Class_{class_id}"
                color = self.colors[class_id % len(self.colors)]
                
                # ë°”ìš´ë”© ë°•ìŠ¤ ê·¸ë¦¬ê¸° (ë” ë‘ê»ê²Œ)
                cv2.rectangle(result_image, (x1, y1), (x2, y2), color, 3)
                
                # ë¼ë²¨ í…ìŠ¤íŠ¸
                label = f"{class_name}: {score:.2f}"
                
                # í…ìŠ¤íŠ¸ í¬ê¸° ê³„ì‚°
                font = cv2.FONT_HERSHEY_SIMPLEX
                font_scale = 0.7
                thickness = 2
                (text_w, text_h), baseline = cv2.getTextSize(label, font, font_scale, thickness)
                
                # ë¼ë²¨ ë°°ê²½ (ë” í° íŒ¨ë”©)
                bg_x1 = x1
                bg_y1 = max(0, y1 - text_h - baseline - 10)
                bg_x2 = min(img_w, x1 + text_w + 10)
                bg_y2 = y1
                
                cv2.rectangle(result_image, (bg_x1, bg_y1), (bg_x2, bg_y2), color, -1)
                
                # ë¼ë²¨ í…ìŠ¤íŠ¸ (í°ìƒ‰ìœ¼ë¡œ ë” ì„ ëª…í•˜ê²Œ)
                text_x = x1 + 5
                text_y = y1 - 5
                cv2.putText(result_image, label, (text_x, text_y), font, font_scale, (255, 255, 255), thickness)
                
            except Exception as e:
                if debug:
                    print(f"   - ë°•ìŠ¤ {i} ê·¸ë¦¬ê¸° ì˜¤ë¥˜: {e}")
                continue
        
        return result_image