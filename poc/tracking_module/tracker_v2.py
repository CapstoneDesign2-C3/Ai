import os
import cv2
import numpy as np
import tensorrt as trt
import pycuda.driver as cuda
import pycuda.autoinit
import torch
import time
from contextlib import contextmanager
from deep_sort_realtime.deepsort_tracker import DeepSort

TRT_LOGGER = trt.Logger(trt.Logger.WARNING)

class OptimizedDetectorTracker:
    def __init__(self, engine_path: str, input_size=(640, 640), conf_thresh=0.25, 
                 max_age=5, max_iou_distance=0.4, nn_budget=50):
        self.input_size = input_size
        self.conf_thresh = conf_thresh
        self.engine = None
        self.context = None
        self.stream = None

        print(f"ğŸš€ Initializing OptimizedDetectorTracker with engine: {engine_path}")

        # ì»¨í…ìŠ¤íŠ¸ ê´€ë¦¬ìë¡œ ì´ˆê¸°í™”
        with self._cuda_context():
            self._load_engine(engine_path)
            self._setup_memory_buffers()
            self._setup_preprocessing_buffers()

        # íŠ¸ë˜ì»¤ ì´ˆê¸°í™”
        self.tracker = DeepSort(max_age=max_age, max_iou_distance=max_iou_distance, nn_budget=nn_budget)

        print("ğŸ›°ï¸ OptimizedDetectorTracker fully initialized.")

    @contextmanager
    def _cuda_context(self):
        """CUDA ì»¨í…ìŠ¤íŠ¸ ìë™ ê´€ë¦¬ (ì•ˆì •ì„± í–¥ìƒ)"""
        try:
            # CUDA ì»¨í…ìŠ¤íŠ¸ ìƒíƒœ í™•ì¸
            current_context = cuda.Context.get_current()
            
            # ìŠ¤íŠ¸ë¦¼ ìƒì„± (ì—†ëŠ” ê²½ìš°ë§Œ)
            if not hasattr(self, 'stream') or self.stream is None:
                self.stream = cuda.Stream()
            
            # ë©”ëª¨ë¦¬ ì •ë¦¬
            if torch.cuda.is_available():
                torch.cuda.empty_cache()
                torch.cuda.synchronize()
            
            # CUDA ì»¨í…ìŠ¤íŠ¸ ë™ê¸°í™”
            cuda.Context.synchronize()
            
            yield
            
        except Exception as e:
            print(f"âš ï¸  CUDA context error: {e}")
            # ì»¨í…ìŠ¤íŠ¸ ë³µêµ¬ ì‹œë„
            try:
                cuda.Context.synchronize()
                if torch.cuda.is_available():
                    torch.cuda.empty_cache()
            except:
                pass
            raise
        finally:
            # ìµœì¢… ë™ê¸°í™”
            try:
                cuda.Context.synchronize()
            except:
                pass

    def _load_engine(self, engine_path: str):
        """TensorRT ì—”ì§„ ë¡œë“œ"""
        try:
            with open(engine_path, 'rb') as f:
                runtime = trt.Runtime(TRT_LOGGER)
                self.engine = runtime.deserialize_cuda_engine(f.read())
            
            if self.engine is None:
                raise RuntimeError(f"âŒ Failed to deserialize engine: {engine_path}")
            
            self.context = self.engine.create_execution_context()
            
            print("âœ… TensorRT Engine loaded successfully")
            print("ğŸ“ Tensor I/O info:")
            for i in range(self.engine.num_io_tensors):
                name = self.engine.get_tensor_name(i)
                shape = self.engine.get_tensor_shape(name)
                dtype = self.engine.get_tensor_dtype(name)
                mode = self.engine.get_tensor_mode(name)
                print(f"   - {name} | shape={shape}, dtype={dtype}, mode={mode}")
                
        except Exception as e:
            print(f"âŒ Failed to load TensorRT engine: {e}")
            raise

    def get_engine_info(self):
        """TensorRT ì—”ì§„ ì •ë³´ ì¶œë ¥ìš© (ë””ë²„ê¹…ìš©)"""
        return {
            'input_size': self.input_size,
            'conf_thresh': self.conf_thresh,
            'num_inputs': len(self.inputs) if hasattr(self, 'inputs') else 0,
            'num_outputs': len(self.outputs) if hasattr(self, 'outputs') else 0
        }

    def _setup_memory_buffers(self):
        """ë©”ëª¨ë¦¬ ë²„í¼ ì„¤ì • ìµœì í™” (cuTensor ì˜¤ë¥˜ ë°©ì§€)"""
        self.bindings, self.inputs, self.outputs = [], [], []

        for name in self.engine:
            dtype = trt.nptype(self.engine.get_tensor_dtype(name))
            shape = self.context.get_tensor_shape(name)
            size = trt.volume(shape)
            
            # ë©”ëª¨ë¦¬ ì •ë ¬ì„ ìœ„í•´ í¬ê¸° ì¡°ì • (512ë°”ì´íŠ¸ ì •ë ¬)
            aligned_size = ((size * dtype().itemsize + 511) // 512) * 512
            dev_mem = cuda.mem_alloc(aligned_size)
            
            self.bindings.append(int(dev_mem))
            mode = self.engine.get_tensor_mode(name)
            
            if mode == trt.TensorIOMode.INPUT:
                self.inputs.append((name, dev_mem, size, dtype))
                # í˜ì´ì§€ ë½ ë©”ëª¨ë¦¬ë„ ì •ë ¬í•˜ì—¬ í• ë‹¹
                self.input_host_mem = cuda.pagelocked_empty(size, dtype)
            else:
                self.outputs.append((name, dev_mem, size, dtype))
                self.output_host_mem = cuda.pagelocked_empty(size, dtype)

    def _setup_preprocessing_buffers(self):
        """ì „ì²˜ë¦¬ìš© ë²„í¼ ì‚¬ì „ í• ë‹¹"""
        h, w = self.input_size
        # ì¬ì‚¬ìš© ê°€ëŠ¥í•œ ë²„í¼ë“¤
        self.resized_buffer = np.empty((h, w, 3), dtype=np.uint8)
        self.normalized_buffer = np.empty((h, w, 3), dtype=np.float32)
        self.transposed_buffer = np.empty((1, 3, h, w), dtype=np.float32)

    def preprocess_optimized(self, frame: np.ndarray):
        """ìµœì í™”ëœ ì „ì²˜ë¦¬"""
        h, w = self.input_size
        self.orig_h, self.orig_w = frame.shape[:2]
        
        # ê¸°ì¡´ ë²„í¼ ì¬ì‚¬ìš©
        cv2.resize(frame, (w, h), dst=self.resized_buffer)
        cv2.cvtColor(self.resized_buffer, cv2.COLOR_BGR2RGB, dst=self.resized_buffer)
        
        # ì •ê·œí™” (in-place ì—°ì‚°)
        np.divide(self.resized_buffer, 255.0, out=self.normalized_buffer)
        
        # ì°¨ì› ë³€ê²½
        self.transposed_buffer[0] = np.transpose(self.normalized_buffer, (2, 0, 1))
        
        return self.transposed_buffer

    def postprocess_vectorized(self, output: np.ndarray, frame_shape):
        """ë²¡í„°í™”ëœ í›„ì²˜ë¦¬ (HALF ë°ì´í„° íƒ€ì… ì§€ì›)"""
        h_img, w_img = frame_shape[:2]
        try:
            # HALF íƒ€ì…ì—ì„œ FLOAT32ë¡œ ë³€í™˜
            if output.dtype == np.float16:
                output = output.astype(np.float32)
            
            # ì¶œë ¥ í˜•íƒœ ì •ê·œí™” (YOLO11m ì¶œë ¥: [1, 300, 6])
            if len(output.shape) == 1:
                # 1D ë°°ì—´ì¸ ê²½ìš° reshaping
                expected_elements = 300 * 6  # YOLO11m ì¶œë ¥ í¬ê¸°
                if output.size >= expected_elements:
                    data = output[:expected_elements].reshape(300, 6)
                else:
                    return []
            elif len(output.shape) == 3 and output.shape[0] == 1:
                # [1, 300, 6] í˜•íƒœ
                data = output[0]  # [300, 6]
            elif len(output.shape) == 2:
                # [300, 6] í˜•íƒœ
                data = output
            else:
                print(f"[Warning] Unexpected output shape: {output.shape}")
                return []

            if data.size == 0 or data.shape[1] < 6:
                return []

            # YOLO11m ì¶œë ¥ í˜•íƒœ: [x1, y1, x2, y2, conf, class]
            boxes = data[:, :4]  # bbox ì¢Œí‘œ
            confidences = data[:, 4]  # confidence scores
            class_ids = data[:, 5].astype(int)  # class IDs
            
            # Person í´ë˜ìŠ¤ (ID: 0)ë§Œ í•„í„°ë§í•˜ê³  confidence ì„ê³„ê°’ ì ìš©
            keep = (confidences > self.conf_thresh) & (class_ids == 0)

            if not np.any(keep):
                return []

            # í•„í„°ë§ëœ ë°ì´í„°
            filtered_boxes = boxes[keep]
            filtered_scores = confidences[keep]
            filtered_cls_ids = class_ids[keep]

            # ì¢Œí‘œ ìŠ¤ì¼€ì¼ë§ (YOLO11mì€ ì´ë¯¸ ì ˆëŒ€ ì¢Œí‘œ)
            input_h, input_w = self.input_size
            scale_x = w_img / input_w
            scale_y = h_img / input_h

            # ì¢Œí‘œ ë³€í™˜ ë° ìŠ¤ì¼€ì¼ë§
            x1, y1, x2, y2 = filtered_boxes.T
            x1_scaled = np.clip((x1 * scale_x).astype(int), 0, w_img - 1)
            y1_scaled = np.clip((y1 * scale_y).astype(int), 0, h_img - 1)
            x2_scaled = np.clip((x2 * scale_x).astype(int), 0, w_img - 1)
            y2_scaled = np.clip((y2 * scale_y).astype(int), 0, h_img - 1)

            # ìœ íš¨í•œ ë°•ìŠ¤ í•„í„°ë§
            valid_mask = (x2_scaled > x1_scaled + 5) & (y2_scaled > y1_scaled + 5)
            
            # ê²°ê³¼ ìƒì„±
            results = []
            valid_indices = np.where(valid_mask)[0]
            
            for i in valid_indices:
                results.append({
                    'bbox': [x1_scaled[i], y1_scaled[i], x2_scaled[i], y2_scaled[i]],
                    'score': float(filtered_scores[i]),
                    'class': int(filtered_cls_ids[i])
                })

            return results

        except Exception as e:
            print(f"[Error] postprocess_vectorized failed: {e}")
            print(f"[Debug] Output shape: {output.shape}, dtype: {output.dtype}")
            return []

    def detect_and_track(self, frame: np.ndarray):
        """ìµœì í™”ëœ ê²€ì¶œ ë° ì¶”ì  (cuTensor ì˜¤ë¥˜ ë°©ì§€)"""
        try:
            with self._cuda_context():
                # ìµœì í™”ëœ ì „ì²˜ë¦¬
                tensor = self.preprocess_optimized(frame)
                np.copyto(self.input_host_mem, tensor.ravel().astype(np.float16))  # HALF íƒ€ì…ìœ¼ë¡œ ë³€í™˜

                # ë™ê¸° ë°©ì‹ìœ¼ë¡œ ë³€ê²½ (ì•ˆì •ì„± í–¥ìƒ)
                in_name, in_ptr, _, _ = self.inputs[0]
                out_name, out_ptr, _, _ = self.outputs[0]

                # ë™ê¸° ë©”ëª¨ë¦¬ ë³µì‚¬
                cuda.memcpy_htod(in_ptr, self.input_host_mem)
                
                # í…ì„œ ì£¼ì†Œ ì„¤ì •
                self.context.set_tensor_address(in_name, in_ptr)
                self.context.set_tensor_address(out_name, out_ptr)
                
                # CUDA ì»¨í…ìŠ¤íŠ¸ ë™ê¸°í™”
                cuda.Context.synchronize()
                
                # ë™ê¸° ì‹¤í–‰ ì‹œë„
                try:
                    success = self.context.execute_v2(self.bindings)
                    if not success:
                        print("[Warning] TensorRT execution failed, trying alternative method")
                        # ëŒ€ì•ˆ: ë¹„ë™ê¸° ì‹¤í–‰
                        self.context.execute_async_v3(self.stream.handle)
                        self.stream.synchronize()
                except Exception as exec_error:
                    print(f"[Warning] Primary execution failed: {exec_error}")
                    # ëŒ€ì•ˆ: ë¹„ë™ê¸° ì‹¤í–‰
                    self.context.execute_async_v3(self.stream.handle)
                    self.stream.synchronize()
                
                # ê²°ê³¼ ë³µì‚¬
                cuda.memcpy_dtoh(self.output_host_mem, out_ptr)
                
                # í›„ì²˜ë¦¬ (HALF íƒ€ì… ì²˜ë¦¬)
                output_data = self.output_host_mem.astype(np.float32)  # HALF -> FLOAT32 ë³€í™˜
                detections = self.postprocess_vectorized(output_data, frame.shape)

                # ì¶”ì  ì—…ë°ì´íŠ¸
                det_list = [([*d['bbox']], d['score'], d['class']) for d in detections]
                tracks = self.tracker.update_tracks(det_list, frame=frame)

                # ê²°ê³¼ ìƒì„±
                results = []
                for track in tracks:
                    if not track.is_confirmed():
                        continue

                    l, t, r, b = track.to_ltrb()
                    results.append({
                        'local_id': getattr(track, 'track_id', None),
                        'bbox': [int(l), int(t), int(r), int(b)],
                        'score': getattr(track, 'det_conf', 0.5),
                        'class': getattr(track, 'det_class', 0),
                        'is_new': getattr(track, 'age', 0) == 1
                    })
                
                return results

        except Exception as e:
            print(f"[Error] detect_and_track failed: {e}")
            # CUDA ì»¨í…ìŠ¤íŠ¸ ì¬ì„¤ì • ì‹œë„
            try:
                cuda.Context.synchronize()
            except:
                pass
            return []

    def visualize_results(self, frame: np.ndarray, results: list):
        """
        ê²°ê³¼ë¥¼ í”„ë ˆì„ì— ì‹œê°í™” (í–¥ìƒëœ ë²„ì „)
        """
        annotated_frame = frame.copy()

        # ì „ì²´ ì •ë³´ í‘œì‹œ
        info_text = f"Objects: {len(results)}"
        cv2.putText(annotated_frame, info_text, (10, 30), 
                   cv2.FONT_HERSHEY_SIMPLEX, 0.8, (255, 255, 255), 2)

        for result in results:
            try:
                bbox = result.get('bbox', [])
                if len(bbox) != 4:
                    continue
                    
                x1, y1, x2, y2 = bbox
                
                # ìƒˆ ê°ì²´ì™€ ê¸°ì¡´ ê°ì²´ êµ¬ë¶„
                is_new = result.get('is_new', False)
                color = (0, 255, 255) if is_new else (0, 255, 0)  # ë…¸ë€ìƒ‰: ìƒˆ ê°ì²´, ì´ˆë¡ìƒ‰: ê¸°ì¡´ ê°ì²´
                thickness = 3 if is_new else 2
                
                # ë°”ìš´ë”© ë°•ìŠ¤ ê·¸ë¦¬ê¸°
                cv2.rectangle(annotated_frame, (x1, y1), (x2, y2), color, thickness)
                
                # ë¼ë²¨ í…ìŠ¤íŠ¸ êµ¬ì„±
                label_parts = ["Person"]
                
                # ID ì¶”ê°€
                local_id = result.get('local_id')
                if local_id is not None:
                    label_parts.append(f"ID:{local_id}")
                
                # ì ìˆ˜ ì¶”ê°€
                score = result.get('score')
                if score is not None:
                    label_parts.append(f"{score:.2f}")
                
                # NEW í‘œì‹œ
                if is_new:
                    label_parts.append("NEW")
                
                label = " ".join(label_parts)
                
                # ë¼ë²¨ ë°°ê²½ (ë” ë„“ê²Œ)
                label_size = cv2.getTextSize(label, cv2.FONT_HERSHEY_SIMPLEX, 0.6, 2)[0]
                bg_color = (0, 255, 255) if is_new else (0, 255, 0)
                
                cv2.rectangle(annotated_frame, 
                             (x1, y1 - label_size[1] - 12), 
                             (x1 + label_size[0] + 10, y1), 
                             bg_color, -1)
                
                # ë¼ë²¨ í…ìŠ¤íŠ¸ (ê²€ì€ìƒ‰ìœ¼ë¡œ ì„ ëª…í•˜ê²Œ)
                cv2.putText(annotated_frame, label, (x1 + 5, y1 - 6), 
                           cv2.FONT_HERSHEY_SIMPLEX, 0.6, (0, 0, 0), 2)
                
                # ì¤‘ì‹¬ì  í‘œì‹œ (ì„ íƒì )
                center_x = (x1 + x2) // 2
                center_y = (y1 + y2) // 2
                cv2.circle(annotated_frame, (center_x, center_y), 3, color, -1)
                
            except Exception as viz_error:
                print(f"âš ï¸  Visualization error for result: {viz_error}")
                continue
        
        # íƒ€ì„ìŠ¤íƒ¬í”„ ì¶”ê°€
        timestamp = time.strftime("%Y-%m-%d %H:%M:%S")
        cv2.putText(annotated_frame, timestamp, 
                   (10, annotated_frame.shape[0] - 10), 
                   cv2.FONT_HERSHEY_SIMPLEX, 0.5, (255, 255, 255), 1)
        
        return annotated_frame
    
    def benchmark_performance(self, frame: np.ndarray, iterations=100):
        """ì„±ëŠ¥ ë²¤ì¹˜ë§ˆí¬ (ì»¨í…ìŠ¤íŠ¸ ê´€ë¦¬ í¬í•¨)"""
        print(f"ğŸ”¥ Performance Benchmark ({iterations} iterations)")
        
        # ì›Œë°ì—…
        for _ in range(10):
            self.detect_and_track(frame)
        
        # ë²¤ì¹˜ë§ˆí¬
        start_time = time.perf_counter()
        for _ in range(iterations):
            results = self.detect_and_track(frame)
        end_time = time.perf_counter()
        
        avg_time = (end_time - start_time) / iterations
        fps = 1.0 / avg_time
        
        print(f"âš¡ Average inference time: {avg_time*1000:.2f} ms")
        print(f"ğŸ¯ Average FPS: {fps:.1f}")
        
        return avg_time, fps

    def profile_pipeline(self, frame: np.ndarray):
        """íŒŒì´í”„ë¼ì¸ í”„ë¡œíŒŒì¼ë§ (ì»¨í…ìŠ¤íŠ¸ ê´€ë¦¬ í¬í•¨)"""
        print("ğŸ” Pipeline Profiling")
        
        with self._cuda_context():
            # ì „ì²˜ë¦¬ ì¸¡ì •
            start = time.perf_counter()
            tensor = self.preprocess_optimized(frame)
            preprocess_time = time.perf_counter() - start
            
            # ì¶”ë¡  ì¸¡ì •
            start = time.perf_counter()
            np.copyto(self.input_host_mem, tensor.ravel())
            in_name, in_ptr, _, _ = self.inputs[0]
            out_name, out_ptr, _, _ = self.outputs[0]
            cuda.memcpy_htod_async(in_ptr, self.input_host_mem, self.stream)
            self.context.set_tensor_address(in_name, in_ptr)
            self.context.set_tensor_address(out_name, out_ptr)
            self.context.execute_async_v3(self.stream.handle)
            self.stream.synchronize()
            inference_time = time.perf_counter() - start
            
            # í›„ì²˜ë¦¬ ì¸¡ì •
            start = time.perf_counter()
            cuda.memcpy_dtoh(self.output_host_mem, out_ptr)
            detections = self.postprocess_vectorized(self.output_host_mem, frame.shape)
            postprocess_time = time.perf_counter() - start
        
        print(f"ğŸ“Š Preprocessing: {preprocess_time*1000:.2f} ms")
        print(f"ğŸš€ Inference: {inference_time*1000:.2f} ms") 
        print(f"âš™ï¸  Postprocessing: {postprocess_time*1000:.2f} ms")
        print(f"ğŸ¯ Total: {(preprocess_time + inference_time + postprocess_time)*1000:.2f} ms")

    def cleanup_resources(self):
        """ë¦¬ì†ŒìŠ¤ ì •ë¦¬ ë©”ì„œë“œ"""
        try:
            print("ğŸ§¹ Cleaning up OptimizedDetectorTracker resources...")
            
            # GPU ë©”ëª¨ë¦¬ í•´ì œ
            if hasattr(self, 'inputs'):
                for _, dev_mem, _, _ in self.inputs:
                    if dev_mem:
                        dev_mem.free()
            
            if hasattr(self, 'outputs'):
                for _, dev_mem, _, _ in self.outputs:
                    if dev_mem:
                        dev_mem.free()
            
            # ìŠ¤íŠ¸ë¦¼ ì •ë¦¬
            if hasattr(self, 'stream') and self.stream:
                self.stream.synchronize()
            
            # CUDA ìºì‹œ ì •ë¦¬
            if torch.cuda.is_available():
                torch.cuda.empty_cache()
                torch.cuda.synchronize()
            
            print("âœ… OptimizedDetectorTracker cleanup completed")
            
        except Exception as e:
            print(f"[Warning] Cleanup error: {e}")

    def __del__(self):
        """ë©”ëª¨ë¦¬ ì •ë¦¬"""
        self.cleanup_resources()

    def __enter__(self):
        """ì»¨í…ìŠ¤íŠ¸ ë§¤ë‹ˆì € ì§„ì…"""
        return self

    def __exit__(self, exc_type, exc_val, exc_tb):
        """ì»¨í…ìŠ¤íŠ¸ ë§¤ë‹ˆì € ì¢…ë£Œ"""
        self.cleanup_resources()
        return False