import numpy as np
import cv2
import tensorrt as trt
import pycuda.driver as cuda
import pycuda.autoinit
import time
import json
from pathlib import Path

class TensorRTYOLOv11:
    def __init__(self, engine_path, class_names_path=None, conf_threshold=0.25, iou_threshold=0.45):
        """
        TensorRT YOLOv11 ì—”ì§„ ì´ˆê¸°í™”
        
        Args:
            engine_path: .engine íŒŒì¼ ê²½ë¡œ
            class_names_path: í´ë˜ìŠ¤ ì´ë¦„ íŒŒì¼ ê²½ë¡œ (ì„ íƒì‚¬í•­)
            conf_threshold: ì‹ ë¢°ë„ ì„ê³„ê°’
            iou_threshold: IoU ì„ê³„ê°’
        """
        self.engine_path = engine_path
        self.conf_threshold = conf_threshold
        self.iou_threshold = iou_threshold
        
        # í´ë˜ìŠ¤ ì´ë¦„ ë¡œë“œ
        self.class_names = self._load_class_names(class_names_path)
        self.colors = np.random.uniform(0, 255, size=(len(self.class_names), 3))
        
        # TensorRT ì—”ì§„ ë¡œë“œ
        self.logger = trt.Logger(trt.Logger.WARNING)
        self.engine = self._load_engine()
        self.context = self.engine.create_execution_context()
        
        # ì…ì¶œë ¥ ë°”ì¸ë”© ì„¤ì •
        self.inputs, self.outputs, self.bindings, self.stream = self._allocate_buffers()
        
        # ëª¨ë¸ ì •ë³´ ì¶œë ¥
        self._print_engine_info()
    
    def _load_class_names(self, class_names_path):
        """í´ë˜ìŠ¤ ì´ë¦„ ë¡œë“œ"""
        if class_names_path and Path(class_names_path).exists():
            try:
                with open(class_names_path, 'r', encoding='utf-8') as f:
                    if class_names_path.endswith('.json'):
                        data = json.load(f)
                        return data.get('names', list(data.values()))
                    else:
                        return [line.strip() for line in f.readlines() if line.strip()]
            except Exception as e:
                print(f"âš ï¸  í´ë˜ìŠ¤ íŒŒì¼ ë¡œë“œ ì‹¤íŒ¨: {e}")
        
        # COCO 80ê°œ í´ë˜ìŠ¤ (YOLOv11 ê¸°ë³¸)
        return [
            'person', 'bicycle', 'car', 'motorcycle', 'airplane', 'bus', 'train', 'truck', 'boat',
            'traffic light', 'fire hydrant', 'stop sign', 'parking meter', 'bench', 'bird', 'cat',
            'dog', 'horse', 'sheep', 'cow', 'elephant', 'bear', 'zebra', 'giraffe', 'backpack',
            'umbrella', 'handbag', 'tie', 'suitcase', 'frisbee', 'skis', 'snowboard', 'sports ball',
            'kite', 'baseball bat', 'baseball glove', 'skateboard', 'surfboard', 'tennis racket',
            'bottle', 'wine glass', 'cup', 'fork', 'knife', 'spoon', 'bowl', 'banana', 'apple',
            'sandwich', 'orange', 'broccoli', 'carrot', 'hot dog', 'pizza', 'donut', 'cake',
            'chair', 'couch', 'potted plant', 'bed', 'dining table', 'toilet', 'tv', 'laptop',
            'mouse', 'remote', 'keyboard', 'cell phone', 'microwave', 'oven', 'toaster', 'sink',
            'refrigerator', 'book', 'clock', 'vase', 'scissors', 'teddy bear', 'hair drier',
            'toothbrush'
        ]
    
    def _load_engine(self):
        """TensorRT ì—”ì§„ ë¡œë“œ"""
        try:
            with open(self.engine_path, 'rb') as f, trt.Runtime(self.logger) as runtime:
                engine = runtime.deserialize_cuda_engine(f.read())
            print(f"âœ… TensorRT ì—”ì§„ ë¡œë“œ ì™„ë£Œ: {self.engine_path}")
            return engine
        except Exception as e:
            print(f"âŒ ì—”ì§„ ë¡œë“œ ì‹¤íŒ¨: {e}")
            raise
    
    def _allocate_buffers(self):
        """GPU ë©”ëª¨ë¦¬ í• ë‹¹"""
        inputs = []
        outputs = []
        bindings = []
        stream = cuda.Stream()
        
        # TensorRT ë²„ì „ì— ë”°ë¥¸ ì²˜ë¦¬
        if hasattr(self.engine, 'num_io_tensors'):
            # TensorRT 8.5+ ìƒˆë¡œìš´ API
            for i in range(self.engine.num_io_tensors):
                tensor_name = self.engine.get_tensor_name(i)
                dtype = trt.nptype(self.engine.get_tensor_dtype(tensor_name))
                shape = self.context.get_tensor_shape(tensor_name)
                size = trt.volume(shape)
                
                # GPU ë©”ëª¨ë¦¬ í• ë‹¹
                host_mem = cuda.pagelocked_empty(size, dtype)
                device_mem = cuda.mem_alloc(host_mem.nbytes)
                bindings.append(int(device_mem))
                
                if self.engine.get_tensor_mode(tensor_name) == trt.TensorIOMode.INPUT:
                    self.input_shape = shape
                    self.input_name = tensor_name
                    inputs.append({'host': host_mem, 'device': device_mem, 'name': tensor_name})
                else:
                    self.output_name = tensor_name
                    outputs.append({'host': host_mem, 'device': device_mem, 'shape': shape, 'name': tensor_name})
        else:
            # TensorRT 7.x/8.x ì´ì „ API
            for binding in self.engine:
                dtype = trt.nptype(self.engine.get_binding_dtype(binding))
                shape = self.context.get_binding_shape(binding)
                size = trt.volume(shape)
                
                # GPU ë©”ëª¨ë¦¬ í• ë‹¹
                host_mem = cuda.pagelocked_empty(size, dtype)
                device_mem = cuda.mem_alloc(host_mem.nbytes)
                bindings.append(int(device_mem))
                
                if self.engine.binding_is_input(binding):
                    self.input_shape = shape
                    self.input_name = binding
                    inputs.append({'host': host_mem, 'device': device_mem, 'name': binding})
                else:
                    self.output_name = binding
                    outputs.append({'host': host_mem, 'device': device_mem, 'shape': shape, 'name': binding})
        
        return inputs, outputs, bindings, stream
    
    def _print_engine_info(self):
        """ì—”ì§„ ì •ë³´ ì¶œë ¥"""
        print(f"ğŸ”§ TensorRT ì—”ì§„ ì •ë³´:")
        print(f"   - TensorRT ë²„ì „: {trt.__version__}")
        print(f"   - ì…ë ¥ í¬ê¸°: {self.input_shape}")
        print(f"   - í´ë˜ìŠ¤ ìˆ˜: {len(self.class_names)}")
        print(f"   - ì‹ ë¢°ë„ ì„ê³„ê°’: {self.conf_threshold}")
        print(f"   - IoU ì„ê³„ê°’: {self.iou_threshold}")
        
        # ì‚¬ìš© ê°€ëŠ¥í•œ ë©”ì„œë“œ í™•ì¸
        methods = []
        if hasattr(self.context, 'execute_async_v3'):
            methods.append("execute_async_v3")
        if hasattr(self.context, 'execute_async_v2'):
            methods.append("execute_async_v2")
        if hasattr(self.context, 'execute_async'):
            methods.append("execute_async")
        if hasattr(self.context, 'execute_v2'):
            methods.append("execute_v2")
        if hasattr(self.context, 'execute'):
            methods.append("execute")
        
        print(f"   - ì‚¬ìš© ê°€ëŠ¥í•œ ì‹¤í–‰ ë©”ì„œë“œ: {', '.join(methods)}")
    
    def preprocess(self, image):
        """ì´ë¯¸ì§€ ì „ì²˜ë¦¬"""
        # YOLOv11 ì…ë ¥ í¬ê¸° (ë³´í†µ 640x640)
        input_h, input_w = self.input_shape[2], self.input_shape[3]
        
        # ì´ë¯¸ì§€ ë¦¬ì‚¬ì´ì¦ˆ (ë¹„ìœ¨ ìœ ì§€)
        img_h, img_w = image.shape[:2]
        scale = min(input_w / img_w, input_h / img_h)
        new_w, new_h = int(img_w * scale), int(img_h * scale)
        
        # ë¦¬ì‚¬ì´ì¦ˆ ë° íŒ¨ë”©
        resized = cv2.resize(image, (new_w, new_h))
        padded = np.full((input_h, input_w, 3), 114, dtype=np.uint8)
        
        # ì¤‘ì•™ ë°°ì¹˜
        pad_x = (input_w - new_w) // 2
        pad_y = (input_h - new_h) // 2
        padded[pad_y:pad_y + new_h, pad_x:pad_x + new_w] = resized
        
        # ì •ê·œí™” ë° ì°¨ì› ë³€ê²½ (HWC -> CHW)
        input_tensor = padded.astype(np.float32) / 255.0
        input_tensor = np.transpose(input_tensor, (2, 0, 1))
        input_tensor = np.expand_dims(input_tensor, axis=0)
        
        return input_tensor, scale, pad_x, pad_y
    
    def postprocess(self, outputs, scale, pad_x, pad_y, debug=False):
        """í›„ì²˜ë¦¬ - NMS ì ìš©í•˜ì—¬ ìµœì¢… ê²€ì¶œ ê²°ê³¼ ìƒì„±"""
        if debug:
            print(f"ğŸ” í›„ì²˜ë¦¬ ë””ë²„ê·¸:")
            print(f"   - ì¶œë ¥ ê°œìˆ˜: {len(outputs)}")
            for i, output in enumerate(outputs):
                print(f"   - ì¶œë ¥ {i} í˜•íƒœ: {output.shape}")
        
        # YOLOv11 ì¶œë ¥ í˜•ì‹ í™•ì¸ ë° ì²˜ë¦¬
        output = outputs[0]
        
        # YOLOv11ì€ ë³´í†µ (1, 84, 8400) í˜•ì‹ìœ¼ë¡œ ì¶œë ¥ë¨
        if len(output.shape) == 3:
            output = output[0]  # ë°°ì¹˜ ì°¨ì› ì œê±° -> (84, 8400)
            if output.shape[0] < output.shape[1]:  # (84, 8400) -> (8400, 84)ë¡œ ì „ì¹˜
                output = output.T
        
        if debug:
            print(f"   - ì²˜ë¦¬ëœ ì¶œë ¥ í˜•íƒœ: {output.shape}")
            print(f"   - ì‹ ë¢°ë„ ì„ê³„ê°’: {self.conf_threshold}")
        
        boxes = []
        scores = []
        class_ids = []
        
        # YOLOv11 ì¶œë ¥ íŒŒì‹±
        for i, detection in enumerate(output):
            # YOLOv11 í˜•ì‹: [x_center, y_center, width, height, class0_conf, class1_conf, ...]
            x_center, y_center, width, height = detection[:4]
            class_confs = detection[4:]
            
            # ìµœëŒ€ í´ë˜ìŠ¤ ì‹ ë¢°ë„ ì°¾ê¸°
            max_conf = np.max(class_confs)
            class_id = np.argmax(class_confs)
            
            if debug and i < 5:  # ì²˜ìŒ 5ê°œë§Œ ë””ë²„ê·¸ ì¶œë ¥
                print(f"   - ê²€ì¶œ {i}: conf={max_conf:.3f}, class={class_id}, pos=({x_center:.1f},{y_center:.1f})")
            
            if max_conf > self.conf_threshold:
                # ì¢Œí‘œ ë³€í™˜ (ëª¨ë¸ ì…ë ¥ í¬ê¸° ê¸°ì¤€ -> ì›ë³¸ ì´ë¯¸ì§€ ê¸°ì¤€)
                input_h, input_w = self.input_shape[2], self.input_shape[3]
                
                # ì •ê·œí™”ëœ ì¢Œí‘œë¥¼ í”½ì…€ ì¢Œí‘œë¡œ ë³€í™˜
                x_center = x_center
                y_center = y_center
                width = width
                height = height
                
                # íŒ¨ë”© ë³´ì • ë° ìŠ¤ì¼€ì¼ë§
                x_center = (x_center - pad_x) / scale
                y_center = (y_center - pad_y) / scale
                width = width / scale
                height = height / scale
                
                # ì¤‘ì‹¬ì ì„ ì¢Œìƒë‹¨ ì¢Œí‘œë¡œ ë³€í™˜
                x1 = x_center - width / 2
                y1 = y_center - height / 2
                
                boxes.append([x1, y1, width, height])
                scores.append(float(max_conf))
                class_ids.append(int(class_id))
        
        if debug:
            print(f"   - ì„ê³„ê°’ í†µê³¼ ê°ì²´: {len(boxes)}ê°œ")
        
        if len(boxes) == 0:
            return [], [], []
        
        # NMS ì ìš©
        boxes = np.array(boxes)
        scores = np.array(scores)
        class_ids = np.array(class_ids)
        
        indices = cv2.dnn.NMSBoxes(boxes.tolist(), scores.tolist(), 
                                  self.conf_threshold, self.iou_threshold)
        
        if len(indices) > 0:
            if isinstance(indices, tuple):
                indices = indices[0] if len(indices) > 0 else []
            if len(indices) > 0:
                indices = indices.flatten() if hasattr(indices, 'flatten') else indices
                
                if debug:
                    print(f"   - NMS í›„ ìµœì¢… ê°ì²´: {len(indices)}ê°œ")
                
                return boxes[indices], scores[indices], class_ids[indices]
        
        if debug:
            print(f"   - NMS í›„ ìµœì¢… ê°ì²´: 0ê°œ")
        
        return [], [], []
    
    def infer(self, image, debug=False):
        """ì¶”ë¡  ì‹¤í–‰"""
        # ì „ì²˜ë¦¬
        start_time = time.time()
        input_tensor, scale, pad_x, pad_y = self.preprocess(image)
        preprocess_time = time.time() - start_time
        
        if debug:
            print(f"ğŸ” ì¶”ë¡  ë””ë²„ê·¸:")
            print(f"   - ì…ë ¥ ì´ë¯¸ì§€ í¬ê¸°: {image.shape}")
            print(f"   - ì „ì²˜ë¦¬ëœ í…ì„œ í¬ê¸°: {input_tensor.shape}")
            print(f"   - ìŠ¤ì¼€ì¼: {scale:.3f}, íŒ¨ë”©: ({pad_x}, {pad_y})")
        
        # GPUë¡œ ë°ì´í„° ë³µì‚¬
        start_time = time.time()
        np.copyto(self.inputs[0]['host'], input_tensor.ravel())
        cuda.memcpy_htod_async(self.inputs[0]['device'], self.inputs[0]['host'], self.stream)
        
        # TensorRT ë²„ì „ì— ë”°ë¥¸ ì¶”ë¡  ì‹¤í–‰ (ìµœì í™”ëœ ë²„ì „)
        try:
            # execute_async_v3 ìš°ì„  ì‚¬ìš© (ê°€ì¥ ë¹ ë¦„)
            if hasattr(self.context, 'execute_async_v3'):
                # í…ì„œ ì£¼ì†Œ ì„¤ì •
                self.context.set_tensor_address(self.input_name, self.inputs[0]['device'])
                for output in self.outputs:
                    self.context.set_tensor_address(output['name'], output['device'])
                # ë¹„ë™ê¸° ì‹¤í–‰
                self.context.execute_async_v3(stream_handle=self.stream.handle)
            else:
                # í´ë°±: execute_v2 ì‚¬ìš© (ë™ê¸°)
                self.context.execute_v2(bindings=self.bindings)
        except Exception as e:
            print(f"ì¶”ë¡  ì‹¤í–‰ ì˜¤ë¥˜: {e}")
            # ìµœì¢… í´ë°±
            self.context.execute_v2(bindings=self.bindings)
        
        # ê²°ê³¼ë¥¼ CPUë¡œ ë³µì‚¬
        for output in self.outputs:
            cuda.memcpy_dtoh_async(output['host'], output['device'], self.stream)
        
        self.stream.synchronize()
        inference_time = time.time() - start_time
        
        # í›„ì²˜ë¦¬
        start_time = time.time()
        output_data = [output['host'].reshape(output['shape']) for output in self.outputs]
        
        if debug:
            print(f"   - ì¶”ë¡  ì‹œê°„: {inference_time*1000:.1f}ms")
            for i, data in enumerate(output_data):
                print(f"   - ì¶œë ¥ {i} í†µê³„: min={np.min(data):.3f}, max={np.max(data):.3f}, mean={np.mean(data):.3f}")
        
        boxes, scores, class_ids = self.postprocess(output_data, scale, pad_x, pad_y, debug)
        postprocess_time = time.time() - start_time
        
        return boxes, scores, class_ids, {
            'preprocess': preprocess_time,
            'inference': inference_time,
            'postprocess': postprocess_time,
            'total': preprocess_time + inference_time + postprocess_time
        }
    
    def draw_detections(self, image, boxes, scores, class_ids, debug=False):
        """ê²€ì¶œ ê²°ê³¼ë¥¼ ì´ë¯¸ì§€ì— ê·¸ë¦¬ê¸°"""
        if debug:
            print(f"ğŸ¨ ê·¸ë¦¬ê¸° ë””ë²„ê·¸:")
            print(f"   - ì´ë¯¸ì§€ í¬ê¸°: {image.shape}")
            print(f"   - ë°•ìŠ¤ ê°œìˆ˜: {len(boxes)}")
        
        if len(boxes) == 0:
            if debug:
                print("   - ê·¸ë¦´ ë°•ìŠ¤ê°€ ì—†ìŒ")
            return image
        
        result_image = image.copy()
        
        for i, (box, score, class_id) in enumerate(zip(boxes, scores, class_ids)):
            try:
                x1, y1, w, h = box.astype(int)
                x2, y2 = x1 + w, y1 + h
                
                # ì´ë¯¸ì§€ ê²½ê³„ ë‚´ë¡œ ì œí•œ
                img_h, img_w = image.shape[:2]
                x1 = max(0, min(x1, img_w - 1))
                y1 = max(0, min(y1, img_h - 1))
                x2 = max(0, min(x2, img_w - 1))
                y2 = max(0, min(y2, img_h - 1))
                
                if debug and i < 3:
                    print(f"   - ë°•ìŠ¤ {i}: ({x1},{y1})-({x2},{y2}), í´ë˜ìŠ¤={class_id}, ì ìˆ˜={score:.3f}")
                
                # ë°•ìŠ¤ê°€ ë„ˆë¬´ ì‘ìœ¼ë©´ ìŠ¤í‚µ
                if x2 - x1 < 5 or y2 - y1 < 5:
                    continue
                
                # í´ë˜ìŠ¤ ì´ë¦„ê³¼ ìƒ‰ìƒ
                class_name = self.class_names[class_id] if class_id < len(self.class_names) else f"Class_{class_id}"
                color = self.colors[class_id % len(self.colors)]
                
                # ë°”ìš´ë”© ë°•ìŠ¤ ê·¸ë¦¬ê¸° (ë” ë‘ê»ê²Œ)
                cv2.rectangle(result_image, (x1, y1), (x2, y2), color, 3)
                
                # ë¼ë²¨ í…ìŠ¤íŠ¸
                label = f"{class_name}: {score:.2f}"
                
                # í…ìŠ¤íŠ¸ í¬ê¸° ê³„ì‚°
                font = cv2.FONT_HERSHEY_SIMPLEX
                font_scale = 0.7
                thickness = 2
                (text_w, text_h), baseline = cv2.getTextSize(label, font, font_scale, thickness)
                
                # ë¼ë²¨ ë°°ê²½ (ë” í° íŒ¨ë”©)
                bg_x1 = x1
                bg_y1 = max(0, y1 - text_h - baseline - 10)
                bg_x2 = min(img_w, x1 + text_w + 10)
                bg_y2 = y1
                
                cv2.rectangle(result_image, (bg_x1, bg_y1), (bg_x2, bg_y2), color, -1)
                
                # ë¼ë²¨ í…ìŠ¤íŠ¸ (í°ìƒ‰ìœ¼ë¡œ ë” ì„ ëª…í•˜ê²Œ)
                text_x = x1 + 5
                text_y = y1 - 5
                cv2.putText(result_image, label, (text_x, text_y), font, font_scale, (255, 255, 255), thickness)
                
            except Exception as e:
                if debug:
                    print(f"   - ë°•ìŠ¤ {i} ê·¸ë¦¬ê¸° ì˜¤ë¥˜: {e}")
                continue
        
        return result_image

def test_image(engine_path, image_path, class_names_path=None, debug=False):
    """ì´ë¯¸ì§€ í…ŒìŠ¤íŠ¸"""
    print(f"ğŸ–¼ï¸  ì´ë¯¸ì§€ í…ŒìŠ¤íŠ¸: {image_path}")
    
    # ëª¨ë¸ ë¡œë“œ
    detector = TensorRTYOLOv11(engine_path, class_names_path)
    
    # ì´ë¯¸ì§€ ë¡œë“œ
    image = cv2.imread(image_path)
    if image is None:
        print(f"âŒ ì´ë¯¸ì§€ë¥¼ ë¶ˆëŸ¬ì˜¬ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {image_path}")
        return
    
    print(f"ğŸ“· ì›ë³¸ ì´ë¯¸ì§€ í¬ê¸°: {image.shape}")
    
    # ë””ë²„ê·¸ ëª¨ë“œ í™•ì¸
    if not debug:
        debug_input = input("ë””ë²„ê·¸ ëª¨ë“œ ì‚¬ìš©? (y/N): ").strip().lower()
        debug = debug_input == 'y'
    
    # ì¶”ë¡  ì‹¤í–‰
    boxes, scores, class_ids, timing = detector.infer(image, debug=debug)
    
    # ê²°ê³¼ ì¶œë ¥
    print(f"\nâ±ï¸  ì²˜ë¦¬ ì‹œê°„:")
    print(f"   - ì „ì²˜ë¦¬: {timing['preprocess']*1000:.1f}ms")
    print(f"   - ì¶”ë¡ : {timing['inference']*1000:.1f}ms")
    print(f"   - í›„ì²˜ë¦¬: {timing['postprocess']*1000:.1f}ms")
    print(f"   - ì´ ì‹œê°„: {timing['total']*1000:.1f}ms")
    print(f"ğŸ¯ ê²€ì¶œëœ ê°ì²´: {len(boxes)}ê°œ")
    
    # ê²€ì¶œëœ ê°ì²´ ì •ë³´ ì¶œë ¥
    if len(boxes) > 0:
        print(f"\nğŸ“‹ ê²€ì¶œëœ ê°ì²´ ëª©ë¡:")
        for i, (box, score, class_id) in enumerate(zip(boxes, scores, class_ids)):
            class_name = detector.class_names[class_id] if class_id < len(detector.class_names) else f"Class_{class_id}"
            x, y, w, h = box
            print(f"   {i+1}. {class_name}: {score:.3f} - ìœ„ì¹˜({x:.1f},{y:.1f}), í¬ê¸°({w:.1f}x{h:.1f})")
    
    # ê²€ì¶œ ê²°ê³¼ ì‹œê°í™”
    result_image = detector.draw_detections(image, boxes, scores, class_ids, debug=debug)
    
    # ê²°ê³¼ í‘œì‹œ
    cv2.imshow('TensorRT YOLOv11 Detection', result_image)
    print(f"\nğŸ‘ï¸  ê²°ê³¼ ì´ë¯¸ì§€ë¥¼ í‘œì‹œí•©ë‹ˆë‹¤. ì•„ë¬´ í‚¤ë‚˜ ëˆ„ë¥´ë©´ ì¢…ë£Œë©ë‹ˆë‹¤.")
    cv2.waitKey(0)
    cv2.destroyAllWindows()
    
    # ê²°ê³¼ ì €ì¥ ì˜µì…˜
    save_result = input("ê²°ê³¼ ì´ë¯¸ì§€ë¥¼ ì €ì¥í•˜ì‹œê² ìŠµë‹ˆê¹Œ? (y/N): ").strip().lower()
    if save_result == 'y':
        output_path = f"{Path(image_path).stem}_detected.jpg"
        cv2.imwrite(output_path, result_image)
        print(f"ğŸ’¾ ê²°ê³¼ ì €ì¥ë¨: {output_path}")

def debug_engine_output(engine_path, image_path=None):
    """ì—”ì§„ ì¶œë ¥ í˜•ì‹ ë””ë²„ê¹…"""
    print("ğŸ” TensorRT ì—”ì§„ ì¶œë ¥ ë””ë²„ê¹…")
    print("=" * 50)
    
    # ëª¨ë¸ ë¡œë“œ
    detector = TensorRTYOLOv11(engine_path)
    
    # í…ŒìŠ¤íŠ¸ ì´ë¯¸ì§€ ì¤€ë¹„
    if image_path and Path(image_path).exists():
        image = cv2.imread(image_path)
    else:
        print("ğŸ“· í…ŒìŠ¤íŠ¸ìš© ë”ë¯¸ ì´ë¯¸ì§€ ìƒì„±")
        image = np.random.randint(0, 255, (640, 640, 3), dtype=np.uint8)
    
    print(f"ğŸ–¼ï¸  ì…ë ¥ ì´ë¯¸ì§€ í¬ê¸°: {image.shape}")
    
    # ì „ì²˜ë¦¬
    input_tensor, scale, pad_x, pad_y = detector.preprocess(image)
    print(f"ğŸ”§ ì „ì²˜ë¦¬ ê²°ê³¼:")
    print(f"   - ì…ë ¥ í…ì„œ í¬ê¸°: {input_tensor.shape}")
    print(f"   - ìŠ¤ì¼€ì¼ íŒ©í„°: {scale}")
    print(f"   - íŒ¨ë”©: x={pad_x}, y={pad_y}")
    
    # GPUë¡œ ë°ì´í„° ë³µì‚¬ ë° ì¶”ë¡ 
    np.copyto(detector.inputs[0]['host'], input_tensor.ravel())
    cuda.memcpy_htod_async(detector.inputs[0]['device'], detector.inputs[0]['host'], detector.stream)
    
    # ì¶”ë¡  ì‹¤í–‰
    if hasattr(detector.context, 'execute_async_v3'):
        detector.context.set_tensor_address(detector.input_name, detector.inputs[0]['device'])
        for output in detector.outputs:
            detector.context.set_tensor_address(output['name'], output['device'])
        detector.context.execute_async_v3(stream_handle=detector.stream.handle)
    else:
        detector.context.execute_v2(bindings=detector.bindings)
    
    # ê²°ê³¼ ë³µì‚¬
    for output in detector.outputs:
        cuda.memcpy_dtoh_async(output['host'], output['device'], detector.stream)
    detector.stream.synchronize()
    
    # ì¶œë ¥ ë¶„ì„
    print(f"\nğŸ“Š ëª¨ë¸ ì¶œë ¥ ë¶„ì„:")
    for i, output in enumerate(detector.outputs):
        data = output['host'].reshape(output['shape'])
        print(f"   ì¶œë ¥ {i}:")
        print(f"     - í˜•íƒœ: {data.shape}")
        print(f"     - ë°ì´í„° íƒ€ì…: {data.dtype}")
        print(f"     - ê°’ ë²”ìœ„: {np.min(data):.3f} ~ {np.max(data):.3f}")
        print(f"     - í‰ê· : {np.mean(data):.3f}")
        print(f"     - í‘œì¤€í¸ì°¨: {np.std(data):.3f}")
        
        # ìƒ˜í”Œ ë°ì´í„° ì¶œë ¥
        if len(data.shape) == 2:  # 2D ì¶œë ¥ì¸ ê²½ìš°
            print(f"     - ìƒ˜í”Œ (ì²« 5í–‰, ì²« 10ì—´):")
            sample = data[:5, :10] if data.shape[1] >= 10 else data[:5, :]
            for row in sample:
                print(f"       {[f'{x:.3f}' for x in row]}")
        elif len(data.shape) == 3:  # 3D ì¶œë ¥ì¸ ê²½ìš°
            print(f"     - ìƒ˜í”Œ (ì²« ë²ˆì§¸ ë°°ì¹˜, ì²« 5í–‰, ì²« 10ì—´):")
            sample = data[0, :5, :10] if data.shape[2] >= 10 else data[0, :5, :]
            for row in sample:
                print(f"       {[f'{x:.3f}' for x in row]}")
    
    print(f"\nğŸ¯ ì‹ ë¢°ë„ ì„ê³„ê°’: {detector.conf_threshold}")
    print(f"ğŸ”„ IoU ì„ê³„ê°’: {detector.iou_threshold}")
    
    # ë‹¤ì–‘í•œ ì„ê³„ê°’ìœ¼ë¡œ í…ŒìŠ¤íŠ¸
    print(f"\nğŸ§ª ë‹¤ì–‘í•œ ì‹ ë¢°ë„ ì„ê³„ê°’ í…ŒìŠ¤íŠ¸:")
    thresholds = [0.1, 0.25, 0.5, 0.7, 0.9]
    
    for thresh in thresholds:
        original_thresh = detector.conf_threshold
        detector.conf_threshold = thresh
        
        output_data = [output['host'].reshape(output['shape']) for output in detector.outputs]
        boxes, scores, class_ids = detector.postprocess(output_data, scale, pad_x, pad_y, debug=False)
        
        print(f"   ì„ê³„ê°’ {thresh}: {len(boxes)}ê°œ ê°ì²´ ê²€ì¶œ")
        if len(boxes) > 0:
            max_score = np.max(scores)
            min_score = np.min(scores)
            print(f"     ì ìˆ˜ ë²”ìœ„: {min_score:.3f} ~ {max_score:.3f}")
        
        detector.conf_threshold = original_thresh

def test_webcam(engine_path, class_names_path=None):
    """ì›¹ìº  í…ŒìŠ¤íŠ¸"""
    print("ğŸ“¹ ì›¹ìº  í…ŒìŠ¤íŠ¸ ì‹œì‘ (ESC í‚¤ë¡œ ì¢…ë£Œ)")
    
    # ëª¨ë¸ ë¡œë“œ
    detector = TensorRTYOLOv11(engine_path, class_names_path)
    
    # ì›¹ìº  ì—´ê¸°
    cap = cv2.VideoCapture(0)
    if not cap.isOpened():
        print("âŒ ì›¹ìº ì„ ì—´ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
        return
    
    # ì„±ëŠ¥ ì¸¡ì • ë³€ìˆ˜
    frame_count = 0
    total_time = 0
    fps_history = []
    
    while True:
        ret, frame = cap.read()
        if not ret:
            break
        
        # ì¶”ë¡  ì‹¤í–‰
        boxes, scores, class_ids, timing = detector.infer(frame)
        
        # FPS ê³„ì‚°
        frame_count += 1
        total_time += timing['total']
        current_fps = 1.0 / timing['total'] if timing['total'] > 0 else 0
        fps_history.append(current_fps)
        
        # ìµœê·¼ 30í”„ë ˆì„ í‰ê·  FPS
        if len(fps_history) > 30:
            fps_history.pop(0)
        avg_fps = sum(fps_history) / len(fps_history)
        
        # ê²€ì¶œ ê²°ê³¼ ê·¸ë¦¬ê¸°
        result_frame = detector.draw_detections(frame, boxes, scores, class_ids)
        
        # ì„±ëŠ¥ ì •ë³´ í‘œì‹œ
        info_text = [
            f"FPS: {avg_fps:.1f}",
            f"Objects: {len(boxes)}",
            f"Inference: {timing['inference']*1000:.1f}ms",
            f"Total: {timing['total']*1000:.1f}ms"
        ]
        
        for i, text in enumerate(info_text):
            cv2.putText(result_frame, text, (10, 30 + i * 25), 
                       cv2.FONT_HERSHEY_SIMPLEX, 0.7, (0, 255, 0), 2)
        
        cv2.imshow('TensorRT YOLOv11 Webcam', result_frame)
        
        # ESC í‚¤ë¡œ ì¢…ë£Œ
        if cv2.waitKey(1) & 0xFF == 27:
            break
    
    cap.release()
    cv2.destroyAllWindows()
    
    print(f"ğŸ“Š í…ŒìŠ¤íŠ¸ ì™„ë£Œ:")
    print(f"   - ì´ í”„ë ˆì„: {frame_count}")
    print(f"   - í‰ê·  FPS: {total_time/frame_count if frame_count > 0 else 0:.1f}")

def test_video(engine_path, video_path, class_names_path=None, save_output=False):
    """ë¹„ë””ì˜¤ í…ŒìŠ¤íŠ¸"""
    print(f"ğŸ¬ ë¹„ë””ì˜¤ í…ŒìŠ¤íŠ¸: {video_path}")
    
    # ëª¨ë¸ ë¡œë“œ
    detector = TensorRTYOLOv11(engine_path, class_names_path)
    
    # ë¹„ë””ì˜¤ ì—´ê¸°
    cap = cv2.VideoCapture(video_path)
    if not cap.isOpened():
        print(f"âŒ ë¹„ë””ì˜¤ë¥¼ ì—´ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {video_path}")
        return
    
    # ë¹„ë””ì˜¤ ì •ë³´
    fps = int(cap.get(cv2.CAP_PROP_FPS))
    width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))
    height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))
    total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))
    
    print(f"ğŸ“º ë¹„ë””ì˜¤ ì •ë³´:")
    print(f"   - í•´ìƒë„: {width}x{height}")
    print(f"   - FPS: {fps}")
    print(f"   - ì´ í”„ë ˆì„: {total_frames}")
    print(f"   - ê¸¸ì´: {total_frames/fps:.1f}ì´ˆ")
    
    # ì¶œë ¥ ë¹„ë””ì˜¤ ì„¤ì • (ì„ íƒì‚¬í•­)
    out = None
    if save_output:
        output_path = f"{Path(video_path).stem}_detected.mp4"
        fourcc = cv2.VideoWriter_fourcc(*'mp4v')
        out = cv2.VideoWriter(output_path, fourcc, fps, (width, height))
        print(f"ğŸ’¾ ì¶œë ¥ íŒŒì¼: {output_path}")
    
    # ì„±ëŠ¥ ì¸¡ì • ë³€ìˆ˜
    frame_count = 0
    total_inference_time = 0
    total_processing_time = 0
    fps_history = []
    detection_counts = []
    
    print("\nğŸ¥ ë¹„ë””ì˜¤ ì²˜ë¦¬ ì‹œì‘... (ìŠ¤í˜ì´ìŠ¤ë°”: ì¼ì‹œì •ì§€, ESC: ì¢…ë£Œ)")
    
    try:
        while True:
            ret, frame = cap.read()
            if not ret:
                break
            
            frame_start_time = time.time()
            
            # ì¶”ë¡  ì‹¤í–‰
            boxes, scores, class_ids, timing = detector.infer(frame)
            
            # í†µê³„ ì—…ë°ì´íŠ¸
            frame_count += 1
            total_inference_time += timing['inference']
            current_fps = 1.0 / timing['total'] if timing['total'] > 0 else 0
            fps_history.append(current_fps)
            detection_counts.append(len(boxes))
            
            # ìµœê·¼ 30í”„ë ˆì„ í‰ê·  FPS
            if len(fps_history) > 30:
                fps_history.pop(0)
            avg_fps = sum(fps_history) / len(fps_history)
            
            # ê²€ì¶œ ê²°ê³¼ ê·¸ë¦¬ê¸°
            result_frame = detector.draw_detections(frame, boxes, scores, class_ids)
            
            # ì§„í–‰ë¥  ê³„ì‚°
            progress = (frame_count / total_frames) * 100
            
            # ì •ë³´ ì˜¤ë²„ë ˆì´
            info_overlay = [
                f"Frame: {frame_count}/{total_frames} ({progress:.1f}%)",
                f"FPS: {avg_fps:.1f}",
                f"Objects: {len(boxes)}",
                f"Inference: {timing['inference']*1000:.1f}ms",
                f"Total: {timing['total']*1000:.1f}ms"
            ]
            
            # ì§„í–‰ë¥  ë°” ê·¸ë¦¬ê¸°
            bar_width = width - 40
            bar_height = 10
            bar_x, bar_y = 20, height - 50
            
            # ë°°ê²½
            cv2.rectangle(result_frame, (bar_x, bar_y), (bar_x + bar_width, bar_y + bar_height), (50, 50, 50), -1)
            # ì§„í–‰ë¥ 
            progress_width = int(bar_width * progress / 100)
            cv2.rectangle(result_frame, (bar_x, bar_y), (bar_x + progress_width, bar_y + bar_height), (0, 255, 0), -1)
            
            # í…ìŠ¤íŠ¸ ì •ë³´
            for i, text in enumerate(info_overlay):
                y_pos = 30 + i * 25
                # ë°°ê²½ ë°•ìŠ¤
                text_size = cv2.getTextSize(text, cv2.FONT_HERSHEY_SIMPLEX, 0.6, 2)[0]
                cv2.rectangle(result_frame, (10, y_pos - 20), (20 + text_size[0], y_pos + 5), (0, 0, 0), -1)
                # í…ìŠ¤íŠ¸
                cv2.putText(result_frame, text, (15, y_pos), cv2.FONT_HERSHEY_SIMPLEX, 0.6, (0, 255, 0), 2)
            
            # í™”ë©´ ì¶œë ¥
            cv2.imshow('TensorRT YOLOv11 Video Analysis', result_frame)
            
            # ì¶œë ¥ íŒŒì¼ ì €ì¥
            if out is not None:
                out.write(result_frame)
            
            total_processing_time += time.time() - frame_start_time
            
            # í‚¤ ì…ë ¥ ì²˜ë¦¬
            key = cv2.waitKey(1) & 0xFF
            if key == 27:  # ESC
                print("\nâ¹ï¸  ì‚¬ìš©ìê°€ ì¤‘ë‹¨í–ˆìŠµë‹ˆë‹¤.")
                break
            elif key == 32:  # Space
                print("â¸ï¸  ì¼ì‹œì •ì§€ (ì•„ë¬´ í‚¤ë‚˜ ëˆ„ë¥´ë©´ ê³„ì†)")
                cv2.waitKey(0)
            
            # ì§„í–‰ë¥  ì¶œë ¥ (10% ë‹¨ìœ„)
            if frame_count % (total_frames // 10 + 1) == 0:
                print(f"ğŸ“Š ì§„í–‰ë¥ : {progress:.1f}% - FPS: {avg_fps:.1f} - ê°ì²´: {len(boxes)}ê°œ")
    
    except KeyboardInterrupt:
        print("\nâ¹ï¸  ì²˜ë¦¬ê°€ ì¤‘ë‹¨ë˜ì—ˆìŠµë‹ˆë‹¤.")
    
    finally:
        # ë¦¬ì†ŒìŠ¤ ì •ë¦¬
        cap.release()
        if out is not None:
            out.release()
        cv2.destroyAllWindows()
        
        # ìµœì¢… í†µê³„
        if frame_count > 0:
            avg_inference_fps = frame_count / total_inference_time if total_inference_time > 0 else 0
            avg_processing_fps = frame_count / total_processing_time if total_processing_time > 0 else 0
            avg_detections = sum(detection_counts) / len(detection_counts) if detection_counts else 0
            
            print(f"\nğŸ“ˆ ë¹„ë””ì˜¤ ë¶„ì„ ì™„ë£Œ:")
            print(f"   - ì²˜ë¦¬ëœ í”„ë ˆì„: {frame_count}/{total_frames}")
            print(f"   - í‰ê·  ì¶”ë¡  FPS: {avg_inference_fps:.1f}")
            print(f"   - í‰ê·  ì „ì²´ FPS: {avg_processing_fps:.1f}")
            print(f"   - í‰ê·  ê²€ì¶œ ê°ì²´: {avg_detections:.1f}ê°œ")
            print(f"   - ì´ ì²˜ë¦¬ ì‹œê°„: {total_processing_time:.1f}ì´ˆ")
            
            if save_output:
                print(f"   - ì¶œë ¥ íŒŒì¼ ì €ì¥ë¨: {output_path}")

def test_rtsp_stream(engine_path, rtsp_url, class_names_path=None):
    """RTSP ìŠ¤íŠ¸ë¦¼ í…ŒìŠ¤íŠ¸"""
    print(f"ğŸ“¡ RTSP ìŠ¤íŠ¸ë¦¼ í…ŒìŠ¤íŠ¸: {rtsp_url}")
    
    # ëª¨ë¸ ë¡œë“œ
    detector = TensorRTYOLOv11(engine_path, class_names_path)
    
    # RTSP ìŠ¤íŠ¸ë¦¼ ì—´ê¸°
    cap = cv2.VideoCapture(rtsp_url)
    cap.set(cv2.CAP_PROP_BUFFERSIZE, 1)  # ë²„í¼ í¬ê¸° ìµœì†Œí™” (ì§€ì—° ê°ì†Œ)
    
    if not cap.isOpened():
        print(f"âŒ RTSP ìŠ¤íŠ¸ë¦¼ì„ ì—´ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {rtsp_url}")
        return
    
    print("ğŸ“º RTSP ìŠ¤íŠ¸ë¦¼ ì—°ê²°ë¨. ESCë¡œ ì¢…ë£Œ...")
    
    # ì„±ëŠ¥ ì¸¡ì •
    frame_count = 0
    fps_history = []
    skip_frames = 0  # í”„ë ˆì„ ë“œë¡­ ì¹´ìš´íŠ¸
    
    try:
        while True:
            ret, frame = cap.read()
            if not ret:
                print("ğŸ“¡ ìŠ¤íŠ¸ë¦¼ ì—°ê²° ëŠì–´ì§. ì¬ì—°ê²° ì‹œë„...")
                time.sleep(1)
                continue
            
            frame_count += 1
            
            # ì‹¤ì‹œê°„ ì²˜ë¦¬ë¥¼ ìœ„í•œ í”„ë ˆì„ ìŠ¤í‚µ (ì„ íƒì‚¬í•­)
            if frame_count % 2 == 0:  # ë§¤ 2ë²ˆì§¸ í”„ë ˆì„ë§Œ ì²˜ë¦¬
                skip_frames += 1
                continue
            
            # ì¶”ë¡  ì‹¤í–‰
            boxes, scores, class_ids, timing = detector.infer(frame)
            
            # FPS ê³„ì‚°
            current_fps = 1.0 / timing['total'] if timing['total'] > 0 else 0
            fps_history.append(current_fps)
            if len(fps_history) > 30:
                fps_history.pop(0)
            avg_fps = sum(fps_history) / len(fps_history)
            
            # ê²€ì¶œ ê²°ê³¼ ê·¸ë¦¬ê¸°
            result_frame = detector.draw_detections(frame, boxes, scores, class_ids)
            
            # ìŠ¤íŠ¸ë¦¼ ì •ë³´ í‘œì‹œ
            stream_info = [
                f"RTSP Stream - LIVE",
                f"FPS: {avg_fps:.1f}",
                f"Objects: {len(boxes)}",
                f"Inference: {timing['inference']*1000:.1f}ms",
                f"Skipped: {skip_frames}"
            ]
            
            for i, text in enumerate(stream_info):
                cv2.putText(result_frame, text, (10, 30 + i * 25), 
                           cv2.FONT_HERSHEY_SIMPLEX, 0.7, (0, 255, 0), 2)
            
            cv2.imshow('TensorRT YOLOv11 RTSP Stream', result_frame)
            
            if cv2.waitKey(1) & 0xFF == 27:
                break
                
    except KeyboardInterrupt:
        print("\nâ¹ï¸  ìŠ¤íŠ¸ë¦¼ ì²˜ë¦¬ê°€ ì¤‘ë‹¨ë˜ì—ˆìŠµë‹ˆë‹¤.")
    
    finally:
        cap.release()
        cv2.destroyAllWindows()
        print(f"ğŸ“Š ì²˜ë¦¬ëœ í”„ë ˆì„: {frame_count - skip_frames}, ìŠ¤í‚µëœ í”„ë ˆì„: {skip_frames}")

def batch_video_analysis(engine_path, video_dir, class_names_path=None, save_results=True):
    """ë°°ì¹˜ ë¹„ë””ì˜¤ ë¶„ì„"""
    video_dir = Path(video_dir)
    if not video_dir.exists():
        print(f"âŒ ë””ë ‰í† ë¦¬ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {video_dir}")
        return
    
    # ì§€ì›í•˜ëŠ” ë¹„ë””ì˜¤ í™•ì¥ì
    video_extensions = ['.mp4', '.avi', '.mov', '.mkv', '.wmv', '.flv', '.webm']
    video_files = []
    
    for ext in video_extensions:
        video_files.extend(video_dir.glob(f'*{ext}'))
        video_files.extend(video_dir.glob(f'*{ext.upper()}'))
    
    if not video_files:
        print(f"âŒ ë¹„ë””ì˜¤ íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {video_dir}")
        return
    
    print(f"ğŸ¬ ë°°ì¹˜ ë¹„ë””ì˜¤ ë¶„ì„ ì‹œì‘: {len(video_files)}ê°œ íŒŒì¼")
    
    # ëª¨ë¸ ë¡œë“œ
    detector = TensorRTYOLOv11(engine_path, class_names_path)
    
    results_summary = []
    
    for i, video_file in enumerate(video_files, 1):
        print(f"\nğŸ“¹ [{i}/{len(video_files)}] ì²˜ë¦¬ ì¤‘: {video_file.name}")
        
        try:
            # ê° ë¹„ë””ì˜¤ì— ëŒ€í•´ ê°„ë‹¨í•œ ë¶„ì„
            cap = cv2.VideoCapture(str(video_file))
            if not cap.isOpened():
                print(f"âŒ íŒŒì¼ì„ ì—´ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {video_file}")
                continue
            
            total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))
            fps = cap.get(cv2.CAP_PROP_FPS)
            
            # ìƒ˜í”Œ í”„ë ˆì„ë“¤ë§Œ ë¶„ì„ (ë§¤ 30í”„ë ˆì„ë§ˆë‹¤)
            sample_frames = list(range(0, total_frames, 30))
            detections_per_frame = []
            
            for frame_idx in sample_frames:
                cap.set(cv2.CAP_PROP_POS_FRAMES, frame_idx)
                ret, frame = cap.read()
                if not ret:
                    continue
                
                boxes, scores, class_ids, timing = detector.infer(frame)
                detections_per_frame.append(len(boxes))
            
            cap.release()
            
            # ê²°ê³¼ ìš”ì•½
            avg_detections = sum(detections_per_frame) / len(detections_per_frame) if detections_per_frame else 0
            max_detections = max(detections_per_frame) if detections_per_frame else 0
            
            result = {
                'filename': video_file.name,
                'total_frames': total_frames,
                'fps': fps,
                'duration': total_frames / fps if fps > 0 else 0,
                'sample_frames': len(sample_frames),
                'avg_objects': avg_detections,
                'max_objects': max_detections
            }
            
            results_summary.append(result)
            
            print(f"   - ì´ í”„ë ˆì„: {total_frames}")
            print(f"   - í‰ê·  ê°ì²´: {avg_detections:.1f}ê°œ")
            print(f"   - ìµœëŒ€ ê°ì²´: {max_detections}ê°œ")
            
        except Exception as e:
            print(f"âŒ ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
            continue
    
    # ê²°ê³¼ ì €ì¥
    if save_results and results_summary:
        results_file = video_dir / 'analysis_results.json'
        with open(results_file, 'w', encoding='utf-8') as f:
            json.dump(results_summary, f, indent=2, ensure_ascii=False)
        print(f"\nğŸ’¾ ë¶„ì„ ê²°ê³¼ ì €ì¥ë¨: {results_file}")
    
    # ì „ì²´ ìš”ì•½
    if results_summary:
        total_duration = sum(r['duration'] for r in results_summary)
        avg_objects_overall = sum(r['avg_objects'] for r in results_summary) / len(results_summary)
        
        print(f"\nğŸ“Š ë°°ì¹˜ ë¶„ì„ ì™„ë£Œ:")
        print(f"   - ì²˜ë¦¬ëœ íŒŒì¼: {len(results_summary)}ê°œ")
        print(f"   - ì´ ì˜ìƒ ê¸¸ì´: {total_duration/60:.1f}ë¶„")
        print(f"   - ì „ì²´ í‰ê·  ê°ì²´: {avg_objects_overall:.1f}ê°œ")

def test_benchmark(engine_path, iterations=100):
    """ë²¤ì¹˜ë§ˆí¬ í…ŒìŠ¤íŠ¸"""
    print(f"âš¡ ë²¤ì¹˜ë§ˆí¬ í…ŒìŠ¤íŠ¸ ({iterations}íšŒ ë°˜ë³µ)")
    
    # ëª¨ë¸ ë¡œë“œ
    detector = TensorRTYOLOv11(engine_path)
    
    # ë”ë¯¸ ì´ë¯¸ì§€ ìƒì„±
    dummy_image = np.random.randint(0, 255, (640, 640, 3), dtype=np.uint8)
    
    # ì›Œë°ì—… (GPU ì´ˆê¸°í™”)
    print("ğŸ”¥ ì›Œë°ì—… ì¤‘...")
    for _ in range(10):
        detector.infer(dummy_image)
    
    # ë²¤ì¹˜ë§ˆí¬ ì‹¤í–‰
    print("ğŸ“Š ë²¤ì¹˜ë§ˆí¬ ì‹¤í–‰ ì¤‘...")
    times = []
    
    for i in range(iterations):
        _, _, _, timing = detector.infer(dummy_image)
        times.append(timing['inference'] * 1000)  # ms ë‹¨ìœ„
        
        if (i + 1) % 20 == 0:
            print(f"   ì§„í–‰ë¥ : {i + 1}/{iterations}")
    
    # ê²°ê³¼ ë¶„ì„
    times = np.array(times)
    print(f"\nğŸ“ˆ ë²¤ì¹˜ë§ˆí¬ ê²°ê³¼:")
    print(f"   - í‰ê·  ì¶”ë¡  ì‹œê°„: {np.mean(times):.2f}ms")
    print(f"   - ìµœì†Œ ì¶”ë¡  ì‹œê°„: {np.min(times):.2f}ms")
    print(f"   - ìµœëŒ€ ì¶”ë¡  ì‹œê°„: {np.max(times):.2f}ms")
    print(f"   - í‘œì¤€í¸ì°¨: {np.std(times):.2f}ms")
    print(f"   - í‰ê·  FPS: {1000/np.mean(times):.1f}")

def main():
    """ë©”ì¸ í•¨ìˆ˜"""
    print("ğŸš€ TensorRT YOLOv11 ì—”ì§„ í…ŒìŠ¤íŠ¸")
    print("=" * 50)
    
    # ì—”ì§„ íŒŒì¼ ê²½ë¡œ ì…ë ¥
    engine_path = input("ì—”ì§„ íŒŒì¼ ê²½ë¡œë¥¼ ì…ë ¥í•˜ì„¸ìš” (.engine): ").strip()
    if not engine_path:
        engine_path = "yolov11.engine"
    
    if not Path(engine_path).exists():
        print(f"âŒ ì—”ì§„ íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {engine_path}")
        return
    
    # í´ë˜ìŠ¤ ì´ë¦„ íŒŒì¼ (ì„ íƒì‚¬í•­)
    class_names_path = input("í´ë˜ìŠ¤ ì´ë¦„ íŒŒì¼ ê²½ë¡œ (ì„ íƒì‚¬í•­, Enterë¡œ ê¸°ë³¸ê°’): ").strip()
    if class_names_path and not Path(class_names_path).exists():
        print(f"âš ï¸  í´ë˜ìŠ¤ íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ì–´ ê¸°ë³¸ê°’ì„ ì‚¬ìš©í•©ë‹ˆë‹¤: {class_names_path}")
        class_names_path = None
    
    # í…ŒìŠ¤íŠ¸ ì˜µì…˜
    print("\ní…ŒìŠ¤íŠ¸ ì˜µì…˜:")
    print("1. ì´ë¯¸ì§€ í…ŒìŠ¤íŠ¸")
    print("2. ì›¹ìº  í…ŒìŠ¤íŠ¸")
    print("3. ë¹„ë””ì˜¤ í…ŒìŠ¤íŠ¸")
    print("4. RTSP ìŠ¤íŠ¸ë¦¼ í…ŒìŠ¤íŠ¸")
    print("5. ë°°ì¹˜ ë¹„ë””ì˜¤ ë¶„ì„")
    print("6. ë²¤ì¹˜ë§ˆí¬ í…ŒìŠ¤íŠ¸")
    
    choice = input("ì„ íƒ (1-6): ").strip()
    
    try:
        if choice == '1':
            image_path = input("ì´ë¯¸ì§€ ê²½ë¡œ: ").strip()
            test_image(engine_path, image_path, class_names_path)
            
        elif choice == '2':
            test_webcam(engine_path, class_names_path)
            
        elif choice == '3':
            video_path = input("ë¹„ë””ì˜¤ íŒŒì¼ ê²½ë¡œ: ").strip()
            save_output = input("ê²°ê³¼ ë¹„ë””ì˜¤ ì €ì¥? (y/N): ").strip().lower() == 'y'
            test_video(engine_path, video_path, class_names_path, save_output)
            
        elif choice == '4':
            rtsp_url = input("RTSP URL (ì˜ˆ: rtsp://192.168.1.100:554/stream): ").strip()
            test_rtsp_stream(engine_path, rtsp_url, class_names_path)
            
        elif choice == '5':
            video_dir = input("ë¹„ë””ì˜¤ í´ë” ê²½ë¡œ: ").strip()
            save_results = input("ë¶„ì„ ê²°ê³¼ JSON ì €ì¥? (Y/n): ").strip().lower() != 'n'
            batch_video_analysis(engine_path, video_dir, class_names_path, save_results)
            
        elif choice == '6':
            iterations = input("ë°˜ë³µ íšŸìˆ˜ (ê¸°ë³¸: 100): ").strip()
            iterations = int(iterations) if iterations.isdigit() else 100
            test_benchmark(engine_path, iterations)
            
        else:
            print("ì˜ëª»ëœ ì„ íƒì…ë‹ˆë‹¤.")
            
    except KeyboardInterrupt:
        print("\nâ¹ï¸  í…ŒìŠ¤íŠ¸ê°€ ì¤‘ë‹¨ë˜ì—ˆìŠµë‹ˆë‹¤.")
    except Exception as e:
        print(f"âŒ ì˜¤ë¥˜ ë°œìƒ: {e}")
        import traceback
        traceback.print_exc()

if __name__ == "__main__":
    main()