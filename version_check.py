#!/usr/bin/env python3
"""
ì™„ì „í•œ CUDA & TensorRT ë²„ì „ í™•ì¸ ìŠ¤í¬ë¦½íŠ¸
ì‚¬ìš©ë²•: python3 check_versions.py
"""

import subprocess
import sys
import os

def run_command(cmd):
    """ëª…ë ¹ì–´ ì‹¤í–‰ ë° ê²°ê³¼ ë°˜í™˜"""
    try:
        result = subprocess.run(cmd, shell=True, capture_output=True, text=True)
        return result.stdout.strip() if result.returncode == 0 else None
    except:
        return None

def check_cuda_versions():
    """CUDA ë²„ì „ë“¤ í™•ì¸"""
    print("ğŸš€ CUDA í™˜ê²½ ì •ë³´")
    print("=" * 50)
    
    # NVCC ë²„ì „ (ì»´íŒŒì¼ëŸ¬)
    nvcc_version = run_command("nvcc --version")
    if nvcc_version:
        # nvcc ì¶œë ¥ì—ì„œ ë²„ì „ ì¶”ì¶œ
        for line in nvcc_version.split('\n'):
            if 'release' in line:
                version = line.split('release ')[1].split(',')[0]
                print(f"âœ… CUDA Compiler (nvcc): {version}")
                break
    else:
        print("âŒ CUDA Compiler (nvcc): ì„¤ì¹˜ë˜ì§€ ì•ŠìŒ")
    
    # NVIDIA-SMI ë²„ì „ (ë“œë¼ì´ë²„)
    nvidia_smi = run_command("nvidia-smi --query-gpu=driver_version,cuda_version --format=csv,noheader,nounits")
    if nvidia_smi:
        parts = nvidia_smi.split(', ')
        if len(parts) >= 2:
            print(f"âœ… NVIDIA Driver: {parts[0]}")
            print(f"âœ… CUDA Runtime (nvidia-smi): {parts[1]}")
    else:
        print("âŒ NVIDIA Driver: ì„¤ì¹˜ë˜ì§€ ì•ŠìŒ ë˜ëŠ” GPU ì—†ìŒ")
    
    # CUDA ì„¤ì¹˜ ê²½ë¡œë“¤ í™•ì¸
    cuda_paths = run_command("ls /usr/local/ | grep cuda")
    if cuda_paths:
        print(f"ğŸ“ CUDA ì„¤ì¹˜ ê²½ë¡œë“¤:")
        for path in cuda_paths.split('\n'):
            if path:
                print(f"   /usr/local/{path}")
    
    # í˜„ì¬ CUDA ì‹¬ë³¼ë¦­ ë§í¬
    cuda_link = run_command("ls -la /usr/local/cuda")
    if cuda_link and '->' in cuda_link:
        target = cuda_link.split('->')[-1].strip()
        print(f"ğŸ”— í˜„ì¬ CUDA ë§í¬: /usr/local/cuda -> {target}")

def check_tensorrt_versions():
    """TensorRT ë²„ì „ë“¤ í™•ì¸"""
    print("\nâš¡ TensorRT í™˜ê²½ ì •ë³´")
    print("=" * 50)
    
    # Python TensorRT ë²„ì „
    try:
        import tensorrt as trt
        print(f"âœ… TensorRT Python: {trt.__version__}")
        
        # TensorRT ë¹Œë” í…ŒìŠ¤íŠ¸
        try:
            logger = trt.Logger(trt.Logger.WARNING)
            builder = trt.Builder(logger)
            print("âœ… TensorRT Builder: ì •ìƒ ì‘ë™")
        except Exception as e:
            print(f"âš ï¸  TensorRT Builder: ì˜¤ë¥˜ - {e}")
            
    except ImportError:
        print("âŒ TensorRT Python: ì„¤ì¹˜ë˜ì§€ ì•ŠìŒ")
    
    # pipìœ¼ë¡œ ì„¤ì¹˜ëœ TensorRT íŒ¨í‚¤ì§€ë“¤
    tensorrt_packages = run_command("pip list | grep -i tensorrt")
    if tensorrt_packages:
        print("ğŸ“¦ ì„¤ì¹˜ëœ TensorRT íŒ¨í‚¤ì§€ë“¤:")
        for package in tensorrt_packages.split('\n'):
            if package:
                print(f"   {package}")
    
    # ì‹œìŠ¤í…œ TensorRT ë¼ì´ë¸ŒëŸ¬ë¦¬
    trt_libs = run_command("find /usr -name '*nvinfer*' -type f 2>/dev/null | head -3")
    if trt_libs:
        print("ğŸ“š TensorRT ë¼ì´ë¸ŒëŸ¬ë¦¬ íŒŒì¼ë“¤:")
        for lib in trt_libs.split('\n'):
            if lib:
                print(f"   {lib}")
    
    # ldconfigì—ì„œ TensorRT í™•ì¸
    trt_ldconfig = run_command("ldconfig -p | grep nvinfer")
    if trt_ldconfig:
        print("ğŸ”— ë¡œë“œëœ TensorRT ë¼ì´ë¸ŒëŸ¬ë¦¬ë“¤:")
        for lib in trt_ldconfig.split('\n')[:3]:  # ìƒìœ„ 3ê°œë§Œ
            if lib:
                print(f"   {lib.strip()}")

def check_pytorch_cuda():
    """PyTorch CUDA í˜¸í™˜ì„± í™•ì¸"""
    print("\nğŸ”¥ PyTorch CUDA ì •ë³´")
    print("=" * 50)
    
    try:
        import torch
        print(f"âœ… PyTorch: {torch.__version__}")
        print(f"âœ… PyTorch CUDA: {torch.version.cuda}")
        print(f"âœ… CUDA ì‚¬ìš© ê°€ëŠ¥: {torch.cuda.is_available()}")
        
        if torch.cuda.is_available():
            print(f"âœ… GPU ê°œìˆ˜: {torch.cuda.device_count()}")
            for i in range(torch.cuda.device_count()):
                print(f"   GPU {i}: {torch.cuda.get_device_name(i)}")
            
            # ë©”ëª¨ë¦¬ ì •ë³´
            memory_allocated = torch.cuda.memory_allocated() / 1024**2
            memory_cached = torch.cuda.memory_reserved() / 1024**2
            print(f"ğŸ“Š GPU ë©”ëª¨ë¦¬: {memory_allocated:.1f}MB ì‚¬ìš©, {memory_cached:.1f}MB ì˜ˆì•½")
            
            # CUDA ì—°ì‚° í…ŒìŠ¤íŠ¸
            try:
                test_tensor = torch.randn(100, 100).cuda()
                result = torch.matmul(test_tensor, test_tensor.T)
                print("âœ… CUDA ì—°ì‚° í…ŒìŠ¤íŠ¸: í†µê³¼")
            except Exception as e:
                print(f"âŒ CUDA ì—°ì‚° í…ŒìŠ¤íŠ¸: ì‹¤íŒ¨ - {e}")
        else:
            print("âŒ CUDAë¥¼ ì‚¬ìš©í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤")
            
    except ImportError:
        print("âŒ PyTorch: ì„¤ì¹˜ë˜ì§€ ì•ŠìŒ")

def check_other_packages():
    """ê¸°íƒ€ ì¤‘ìš” íŒ¨í‚¤ì§€ë“¤ í™•ì¸"""
    print("\nğŸ“¦ ê¸°íƒ€ ì¤‘ìš” íŒ¨í‚¤ì§€ë“¤")
    print("=" * 50)
    
    packages_to_check = [
        ('opencv-python', 'cv2'),
        ('numpy', 'numpy'), 
        ('pycuda', 'pycuda.driver'),
        ('cupy', 'cupy')
    ]
    
    for package_name, import_name in packages_to_check:
        try:
            module = __import__(import_name)
            version = getattr(module, '__version__', 'Unknown')
            print(f"âœ… {package_name}: {version}")
        except ImportError:
            print(f"âŒ {package_name}: ì„¤ì¹˜ë˜ì§€ ì•ŠìŒ")

def check_environment_variables():
    """í™˜ê²½ ë³€ìˆ˜ í™•ì¸"""
    print("\nğŸŒ í™˜ê²½ ë³€ìˆ˜")
    print("=" * 50)
    
    env_vars = [
        'CUDA_HOME',
        'CUDA_PATH', 
        'LD_LIBRARY_PATH',
        'PATH'
    ]
    
    for var in env_vars:
        value = os.environ.get(var, '')
        if var in ['LD_LIBRARY_PATH', 'PATH']:
            # CUDA ê´€ë ¨ ê²½ë¡œë§Œ í•„í„°ë§
            cuda_paths = [path for path in value.split(':') if 'cuda' in path.lower()]
            if cuda_paths:
                print(f"âœ… {var} (CUDA ê´€ë ¨):")
                for path in cuda_paths[:3]:  # ìµœëŒ€ 3ê°œë§Œ
                    print(f"   {path}")
            else:
                print(f"âš ï¸  {var}: CUDA ê²½ë¡œ ì—†ìŒ")
        else:
            if value:
                print(f"âœ… {var}: {value}")
            else:
                print(f"âš ï¸  {var}: ì„¤ì •ë˜ì§€ ì•ŠìŒ")

def main():
    """ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜"""
    print("ğŸ” CUDA & TensorRT í™˜ê²½ ì™„ì „ ë¶„ì„")
    print("=" * 60)
    
    check_cuda_versions()
    check_tensorrt_versions()
    check_pytorch_cuda()
    check_other_packages()
    check_environment_variables()
    
    print(f"\nâœ… í™˜ê²½ ë¶„ì„ ì™„ë£Œ!")
    print("ğŸ“ ì´ ì •ë³´ë¥¼ ë°”íƒ•ìœ¼ë¡œ í˜¸í™˜ íŒ¨í‚¤ì§€ë¥¼ ì„ íƒí•˜ì„¸ìš”.")

if __name__ == "__main__":
    main()